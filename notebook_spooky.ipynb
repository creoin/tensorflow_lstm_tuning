{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Tuning on Spooky Author Identification Dataset\n",
    "\n",
    "This is an example of tuning LSTM architectures with an NLP dataset of exerpts from the writings of three authors. The objective is to tune an LSTM model that can predict the author from a writing exerpt.\n",
    "\n",
    "The steps taken in this notebook are:\n",
    "\n",
    "- Split data into train/valid/test sets\n",
    "- Prepare vocabulary\n",
    "- Tokenise the sentences and labels with IDs\n",
    "- Batching and padding for LSTM input\n",
    "- Define a class that can build an RNN with variable hidden units and layers\n",
    "- Setup training and validation code\n",
    "- Prepare a hyper parameter tuner using Grid Search\n",
    "- Tune an LSTM/GRU over the hyper-parameters for multiple epochs\n",
    "- (Plotting results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, read in the CSV data, and take a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datadir = 'data/spooky_author_identification/'\n",
    "csv_train = os.path.join(datadir, 'train.csv')\n",
    "csv_test  = os.path.join(datadir, 'test.csv')\n",
    "filesdir = 'files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in CSV data\n",
    "def read_csv(filepath, discard_header=False):\n",
    "    data_raw = []\n",
    "    with open(filepath) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            if not row:\n",
    "                continue\n",
    "            if discard_header and i == 0:\n",
    "                continue\n",
    "            data_raw.append(row)\n",
    "    return data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_raw = read_csv(csv_train, discard_header=True)\n",
    "test_raw  = read_csv(csv_test, discard_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Contents\n",
    "\n",
    "Below is a sample of the data. It has been imported here as a list of lists, where each entry contains an ID for the entry, a text phrase as a complete string, and the author initials. The texts are sentence excerpts from the authors books, and there are three authors in the dataset: \n",
    "- EAP: Edgar Allan Poe, \n",
    "- HPL: HP Lovecraft, \n",
    "- MWS: Mary Wollstonecraft Shelley. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id26305    EAP  \n",
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "\n",
      "id17569    HPL  \n",
      "It never once occurred to me that the fumbling might be a mere mistake.\n",
      "\n",
      "id11008    EAP  \n",
      "In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.\n",
      "\n",
      "id27763    MWS  \n",
      "How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text_id, text, author in train_raw[:4]:\n",
    "    print('{:10} {:5}\\n{}\\n'.format(text_id, author, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Quantity\n",
    "\n",
    "There are 19,579 entries in the \"training\" set, and 8,392 entries in the test set. The test set contains only the ID and text, so the author needs to be predicted for submission of results. So the \"training\" set will actually need to be divided into a train and validation set at least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579 entries in training set\n",
      "8392 entries in test set\n"
     ]
    }
   ],
   "source": [
    "print('{} entries in training set'.format(len(train_raw)))\n",
    "print('{} entries in test set'.format(len(test_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data into Train/Valid/Test Datasets\n",
    "\n",
    "Using a previous tool I prepared for the common task of making a train/valid/test split. Copied to this notebook directory to ensure import. Subclass the DataManager class for this particular dataset. In the current implementation, two functions need to be defined to handle the specific processing of the current dataset.\n",
    "\n",
    "`_process_row_raw()`\n",
    "\n",
    "Processes the raw data from the original CSV file. This method treats a single row from the original data format, and outputs a format that will be split into train/valid/test files.\n",
    "\n",
    "`_process_row_split()`\n",
    "\n",
    "This method treats a single row of the train/valid/test files generated with the previous method, and outputs the format that will actually be used for training -- so just the most necessary variables.\n",
    "\n",
    "What the data manager tool does is read in the csv, divide the data by classes, then randomly samples from each class to build up the various sets. Firstly train is filled, then valid, then test. The sets are filled in the proportions specified. This gives each set an even distribution of examples from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_tools as dt\n",
    "from data_tools import DataManager\n",
    "\n",
    "class SpookyData(DataManager):\n",
    "    def __init__(self, filepath, split, one_hot_encode=True, output_numpy=True):\n",
    "        super().__init__(filepath, split, one_hot_encode, output_numpy)\n",
    "        self.filepath = filepath\n",
    "        self.split = split       # train/valid/test fractions, should sum to 1\n",
    "        self.dataset_path = 'data/spooky_author_identification/processed'\n",
    "        self.discard_header = True\n",
    "\n",
    "    def _process_row_raw(self, row):\n",
    "        \"\"\"\n",
    "        Import lines from raw data file.\n",
    "        Imports line of \"ID, text, author\"\n",
    "        Returns list of [ID, text, author, author_index]\n",
    "        \"\"\"\n",
    "        read_line = []\n",
    "        spooky_id    = row[0]\n",
    "        spooky_text  = row[1]\n",
    "        spooky_label = row[2]\n",
    "        label_idx = self._get_idx(spooky_label)\n",
    "        read_line.extend([spooky_id, spooky_text, spooky_label, label_idx])\n",
    "        return read_line\n",
    "\n",
    "    def _process_row_split(self, row):\n",
    "        \"\"\"\n",
    "        Import lines from train/valid/test split files.\n",
    "        Imports line of \"ID, text, author, author_index\"\n",
    "        Returns list of [text, author]\n",
    "        \"\"\"\n",
    "        read_line = []\n",
    "\n",
    "        spooky_text      = row[1]\n",
    "        spooky_label     = row[2]\n",
    "        spooky_label_idx = int(float(row[-1]))\n",
    "        fetch_idx = self._get_idx(spooky_label)  # rebuilds num_classes\n",
    "        read_line.extend([spooky_text, spooky_label])\n",
    "        return read_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above subclass of DataManager handles the specific data structure of the Spooky Author Identification dataset. Now instantiate it with a train/valid/test 80%/10%/10% split. Since we later want to generate a vocabulary, we don't want the labels to be one-hot-encoded yet. We also want python lists as inputs for building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Train/Valid/Test data from data/spooky_author_identification/train.csv\n",
      "Split train has (6320, 4508, 4835) examples of each class\n",
      "Split valid has (790, 563, 604) examples of each class\n",
      "Split test has (790, 564, 605) examples of each class\n",
      "Dataset prepared\n"
     ]
    }
   ],
   "source": [
    "# NLP Spooky Author Identification Dataset\n",
    "filepath = 'data/spooky_author_identification/train.csv'\n",
    "data_manager = dt.SpookyData(filepath, (0.8, 0.1, 0.1), one_hot_encode=False, output_numpy=False)\n",
    "data_manager.init_dataset()\n",
    "train_x, train_y = data_manager.prepare_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Remarks**\n",
    "\n",
    "These counts show that about 40% of the dataset is Edgar Allan Poe, about 29% HP Lovecraft, and 31% Mary wollstonecraft Shelly. The train/valid/test sets have been filled with this ratio. The train/valid/test sets themselves containt 80%/10%/10% of the total dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Vocabulary\n",
    "\n",
    "The input dataset has been processed into train/valid/test divisions, but still contains sentences and labels as input. To be able to process these with an LSTM model, these sentences need to be converted into numbers. This is done by building a vocabulary of the words in a corpus (the training dataset here), where the words are counted and arranged by frequency.\n",
    "\n",
    "Below a Vocabulary class is defined for carrying out this task. Parts of this class will be explained step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "\n",
    "# String to use for padding or unknown token\n",
    "_PAD = \"_PAD\"\n",
    "_UNK = \"_UNK\"\n",
    "\n",
    "# Padding for 0 index, if used\n",
    "START_VOCAB_dict = dict()\n",
    "START_VOCAB_dict['with_padding'] = [_PAD, _UNK]\n",
    "START_VOCAB_dict['no_padding'] = [_UNK]\n",
    "\n",
    "# UNK ID depends if padding is used or not\n",
    "UNK_ID_dict = dict()\n",
    "UNK_ID_dict['with_padding'] = 1\n",
    "UNK_ID_dict['no_padding'] = 0\n",
    "\n",
    "# Regular expressions used to tokenize.\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(r\"\\d\")\n",
    "\n",
    "class Vocabulary(object):\n",
    "    def __init__(self, datadir, max_vocabulary_size):\n",
    "        self.datadir = datadir\n",
    "        if not os.path.exists(self.datadir):\n",
    "            os.makedirs(self.datadir)\n",
    "        self.max_vocabulary_size = max_vocabulary_size\n",
    "        self.corpus_file        = 'sentences_raw.txt'\n",
    "        self.vocab_file         = 'vocab_sentences.txt'\n",
    "        self.label_file         = 'vocab_labels.txt'\n",
    "\n",
    "        # train/valid/test file names\n",
    "        self.sentences_file     = 'sentences.txt'\n",
    "        self.labels_file        = 'labels.txt'\n",
    "        self.ids_sentences_file = 'ids_sentences.txt'\n",
    "        self.ids_labels_file    = 'ids_labels.txt'\n",
    "\n",
    "        self.tokeniser = self.basic_tokeniser\n",
    "\n",
    "        self.vocab_list = []\n",
    "        self.label_list = []\n",
    "\n",
    "        self.vocab_to_id = {}\n",
    "        self.id_to_vocab = []\n",
    "        self.label_to_id = {}\n",
    "        self.id_to_label = []\n",
    "\n",
    "    def build_vocabulary(self, sentences, labels):\n",
    "        self.build_sentence_vocabulary(sentences)\n",
    "        self.build_label_vocabulary(labels)\n",
    "\n",
    "    def build_sentence_vocabulary(self, sentences, normalise_digits=True):\n",
    "        \"\"\"Build vocabulary from a list of sentences\"\"\"\n",
    "        # write sentences (corpus) to file\n",
    "        sentences_raw_path = os.path.join(self.datadir, self.corpus_file)\n",
    "        if not os.path.exists(sentences_raw_path):\n",
    "            print('Creating sentence corpus in {}'.format(sentences_raw_path))\n",
    "            self._write_file_from_list(sentences_raw_path, sentences)\n",
    "\n",
    "        # Build vocabulary from list\n",
    "        print(\"Building vocabulary\")\n",
    "        counter = 0\n",
    "        vocab = {}  # Count occurrences of each word\n",
    "        for sentence in sentences:\n",
    "            counter += 1\n",
    "            if counter % 5000 == 0:\n",
    "                print(\"  processing line {}\".format(counter))\n",
    "            tokens = self.tokeniser(sentence)\n",
    "            for w in tokens:\n",
    "                word = re.sub(_DIGIT_RE, \"0\", w) if normalise_digits else w\n",
    "                if word in vocab:\n",
    "                    vocab[word] += 1\n",
    "                else:\n",
    "                    vocab[word] = 1\n",
    "        vocab_list = START_VOCAB_dict['with_padding'] + sorted(vocab, key=vocab.get, reverse=True)\n",
    "        if len(vocab_list) > self.max_vocabulary_size:\n",
    "            vocab_list = vocab_list[:self.max_vocabulary_size]\n",
    "        self.vocab_list = vocab_list\n",
    "\n",
    "        # Write vocabulary to file\n",
    "        vocab_path = os.path.join(self.datadir, self.vocab_file)\n",
    "        self._write_file_from_list(vocab_path, vocab_list)\n",
    "\n",
    "    def build_label_vocabulary(self, labels):\n",
    "        labels = sorted(set(labels))\n",
    "        self.label_list = labels\n",
    "        label_path = os.path.join(self.datadir, self.label_file)\n",
    "        self._write_file_from_list(label_path, labels)\n",
    "\n",
    "    def get_sentence_vocabulary(self):\n",
    "        vocab, rev_vocab = self.initialise_vocabulary(self.vocab_list)\n",
    "        self.vocab_to_id = vocab\n",
    "        self.id_to_vocab = rev_vocab\n",
    "        return vocab, rev_vocab\n",
    "\n",
    "    def get_label_vocabulary(self):\n",
    "        vocab, rev_vocab = self.initialise_vocabulary(self.label_list)\n",
    "        self.label_to_id = vocab\n",
    "        self.id_to_label = rev_vocab\n",
    "        return vocab, rev_vocab\n",
    "\n",
    "    def initialise_vocabulary(self, vocabulary_list):\n",
    "        \"\"\"\n",
    "        Initialise vocabulary from vocabulary list\n",
    "        Build Vocab > ID dictionary, and ID > Vocab list.\n",
    "        \"\"\"\n",
    "        rev_vocab = [line.strip() for line in vocabulary_list]\n",
    "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "        return vocab, rev_vocab\n",
    "\n",
    "    def basic_tokeniser(self, sentence):\n",
    "        \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "        words = []\n",
    "        for space_separated_fragment in sentence.strip().split():\n",
    "            words.extend(re.split(_WORD_SPLIT, space_separated_fragment))\n",
    "        return [w for w in words if w]\n",
    "\n",
    "    def sentence_to_token_ids(self, sentence, UNK_ID, normalise_digits=True):\n",
    "        words = self.tokeniser(sentence)\n",
    "        if not normalise_digits:\n",
    "            return [self.vocab_to_id.get(w, UNK_ID) for w in words]\n",
    "        # Normalize digits by 0 before looking words up in the vocabulary.\n",
    "        return [self.vocab_to_id.get(re.sub(_DIGIT_RE, \"0\", w), UNK_ID) for w in words]\n",
    "\n",
    "    def data_to_token_ids(self, data, split_name, use_padding=True, normalise_digits=True):\n",
    "        \"\"\"\n",
    "        Converts a list of data into its token IDs, writes\n",
    "        them to a file, and returns the ID form of the data\n",
    "        \"\"\"\n",
    "        tokenised_data = []\n",
    "        for i, line in enumerate(data):\n",
    "            if i % 5000 == 0 and i != 0:\n",
    "                print('  tokenising line {}'.format(i))\n",
    "            if use_padding:\n",
    "                UNK_ID = UNK_ID_dict['with_padding']\n",
    "            else:\n",
    "                UNK_ID = UNK_ID_dict['no_padding']\n",
    "            token_ids = self.sentence_to_token_ids(line, UNK_ID, normalise_digits)\n",
    "            tokenised_data.append(token_ids)\n",
    "\n",
    "        # write file\n",
    "        write_dir = os.path.join(self.datadir, split_name)\n",
    "        if not os.path.exists(write_dir):\n",
    "            os.makedirs(write_dir)\n",
    "        sents_file     = os.path.join(write_dir, self.sentences_file)\n",
    "        ids_sents_file = os.path.join(write_dir, self.ids_sentences_file)\n",
    "\n",
    "        self._write_file_from_list(sents_file, data)\n",
    "        self._write_file_from_list(ids_sents_file, tokenised_data)\n",
    "        return tokenised_data\n",
    "\n",
    "    def labels_to_token_ids(self, labels, split_name):\n",
    "        tokenised_labels = []\n",
    "        for label in labels:\n",
    "            tokenised_labels.append(self.label_to_id[label])\n",
    "\n",
    "        # write file\n",
    "        write_dir = os.path.join(self.datadir, split_name)\n",
    "        if not os.path.exists(write_dir):\n",
    "            os.makedirs(write_dir)\n",
    "        labels_file     = os.path.join(write_dir, self.labels_file)\n",
    "        ids_labels_file = os.path.join(write_dir, self.ids_labels_file)\n",
    "\n",
    "        self._write_file_from_list(labels_file, labels)\n",
    "        self._write_file_from_list(ids_labels_file, tokenised_labels)\n",
    "        return tokenised_labels\n",
    "\n",
    "    def translate_examples(self, examples):\n",
    "        for sentence_tokens, label_token in examples:\n",
    "            sentence_list = []\n",
    "            for token in sentence_tokens:\n",
    "                sentence_list.append(self.id_to_vocab[token])\n",
    "            sentence = \" \".join(sentence_list)\n",
    "            label = self.id_to_label[label_token]\n",
    "            print(\"{}\\n{}\\n\\n\".format(sentence, label))\n",
    "\n",
    "\n",
    "    def _write_file_from_list(self, filename, write_list):\n",
    "        print('Writing {} ...'.format(filename))\n",
    "        with open(filename,'w') as file:\n",
    "            for write_line in write_list:\n",
    "                if not isinstance(write_line, (str, int)):\n",
    "                    write_line = \" \".join([str(item) for item in write_line])\n",
    "                file.write(str(write_line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary\n",
    "\n",
    "Firstly, build the vocabulary with the training split. `build_vocabulary()` calls `build_sentence_vocabulary()` to build vocabulary for the sentences, and `build_label_vocabulary()` for the labels. For the sentences, the sentences are looped through and tokenised (basically split into words, and splitting out punctuation). Then the occurrences of each word are counted, the resulting list sorted (most frequent to least), and finally truncated to the vocabulary size. For the labels it's much more simple, just a sorted set of all the labels in the data, where the set is removing all duplicate occurrences of the labels.\n",
    "\n",
    "Presumably, we're trying to identify an author based on writing style, so word choice and punctuation usage will be characteristic of each author. There may also be indication of genre or expressive style also from the words used. It should be noted here that the words have not been forced to lower case, for example, and capitalisation of words will be considered as separate vocabulary. Forcing everything to lower case could improve accuracy by increasing the vocabulary (different words) and treating words with the same letters as the same entity (unification), however you also lose some information on capitalisation too (which if used enough, could form an author's style).\n",
    "\n",
    "#### Padding and Unknown Tokens\n",
    "\n",
    "There are two special tokens added to the sentence vocabulary. One is a padding token (`_PAD`), which has no meaning, but is used to pad sequences in a batch to the same length. Typically this is at the 0 index, so 0 can be used for padding. Then there is an Unknown token (`_UNK`), which is used for any word that is not in the vocabulary. Here this is at the 1 index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sentence corpus in data/spooky_author_identification/processed/sentences_raw.txt\n",
      "Writing data/spooky_author_identification/processed/sentences_raw.txt ...\n",
      "Building vocabulary\n",
      "  processing line 5000\n",
      "  processing line 10000\n",
      "  processing line 15000\n",
      "Writing data/spooky_author_identification/processed/vocab_sentences.txt ...\n",
      "Writing data/spooky_author_identification/processed/vocab_labels.txt ...\n"
     ]
    }
   ],
   "source": [
    "processed_path = 'data/spooky_author_identification/processed/'\n",
    "vocab = Vocabulary(processed_path, 20000)\n",
    "vocab.build_vocabulary(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token ID and Vocabulary Conversion\n",
    "\n",
    "The organised vocabulary and sentence lists define the contents of the vocabularies, but the ID assignments are done by calling the below methods on the `Vocabulary` object, `get_sentence_vocabulary()` and `get_label_vocabulary()`. These methods call `initialise_vocabulary()` with the corresponding vocabulary/label lists. Then `initialise_vocabulary()` basically steps through the vocabulary list and increments an index, which becomes the ID. The output for each is a dictionary of words to ID (`xx_vocab`), and a list of words (where the index == ID) (`rev_xx_vocab`).\n",
    "\n",
    "These vocabulary conversion dictionaries and lists are also stored in the `Vocabulary` object.\n",
    "\n",
    "Below the first 20 sentence tokens, and the label tokens have been printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 sentence vocabulary:\n",
      "   0   _PAD\n",
      "   1   _UNK\n",
      "   2   ,\n",
      "   3   the\n",
      "   4   of\n",
      "   5   .\n",
      "   6   and\n",
      "   7   to\n",
      "   8   I\n",
      "   9   a\n",
      "  10   in\n",
      "  11   was\n",
      "  12   that\n",
      "  13   ;\n",
      "  14   my\n",
      "  15   \"\n",
      "  16   had\n",
      "  17   with\n",
      "  18   it\n",
      "  19   his\n",
      "\n",
      "\n",
      "Labels:\n",
      "   0   EAP\n",
      "   1   HPL\n",
      "   2   MWS\n"
     ]
    }
   ],
   "source": [
    "sents_vocab, rev_sents_vocab = vocab.get_sentence_vocabulary()\n",
    "label_vocab, rev_label_vocab = vocab.get_label_vocabulary()\n",
    "\n",
    "print('First 20 sentence vocabulary:')\n",
    "for i, word in enumerate(rev_sents_vocab[:20]):\n",
    "    print('{:4d}   {}'.format(i, word))\n",
    "print('\\n\\nLabels:')\n",
    "for i, label in enumerate(rev_label_vocab):\n",
    "    print('{:4d}   {}'.format(i, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenising the Sentences and Labels\n",
    "\n",
    "Here Tokenisation refers to the replacing of words with Token IDs, rather than how the words (tokens) are split.\n",
    "\n",
    "So after our conversions are defined, we can use them to replace all the words/labels with the assigned IDs, turing the input sentence into a sequence of numbers, and the output to one number.\n",
    "\n",
    "The method `data_to_token_ids()` does the sentence conversion, calling `sentence_to_token_ids()` on each sentence that replaces the words with IDs, and also writes the converted data to file. The tokenised data is returned by the method.\n",
    "\n",
    "Similarly, `labels_to_token_ids()` converts the labels into their ID form, writes those to file, and returns the labels IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tokenising line 5000\n",
      "  tokenising line 10000\n",
      "  tokenising line 15000\n",
      "Writing data/spooky_author_identification/processed/train/sentences.txt ...\n",
      "Writing data/spooky_author_identification/processed/train/ids_sentences.txt ...\n",
      "Writing data/spooky_author_identification/processed/train/labels.txt ...\n",
      "Writing data/spooky_author_identification/processed/train/ids_labels.txt ...\n"
     ]
    }
   ],
   "source": [
    "train_x_tok = vocab.data_to_token_ids(train_x, 'train')\n",
    "train_y_tok = vocab.labels_to_token_ids(train_y, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenised Training Set\n",
    "\n",
    "Below we combine the tokenised sentences and labels into a list of examples, `train_set`. \n",
    "\n",
    "The first 3 examples are shown, first in their ID form, and then translating them to their word form with the `Vocabulary.translate_examples()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "([203, 718, 11, 10242, 216, 292, 4, 11861, 13, 6, 18, 194, 19, 1539, 500, 2, 76, 21, 392, 2, 7, 2890, 19, 738, 28, 3, 1548, 23, 52, 32, 3, 484, 4, 35, 4477, 364, 5], 2)\n",
      "\n",
      "Example 1\n",
      "([8955, 159, 245, 43, 19, 706, 2785, 36, 15581, 6, 2730, 2, 6, 21, 1061, 18, 313, 21, 50, 142, 12, 56, 91, 695, 32, 46, 5], 1)\n",
      "\n",
      "Example 2\n",
      "([442, 3, 5226, 11, 1774, 9, 353, 757, 3075, 27, 14, 217, 22, 49, 2, 31, 18, 11, 1376, 12, 780, 6, 8, 110, 2612, 66, 910, 29, 271, 2, 2859, 12, 107, 27, 19295, 6, 6828, 66, 1307, 33, 3, 915, 115, 5], 2)\n",
      "\n",
      "\n",
      "His health was impaired beyond hope of cure ; and it became his earnest wish , before he died , to preserve his daughter from the poverty which would be the portion of her orphan state .\n",
      "MWS\n",
      "\n",
      "\n",
      "Wise men told him his simple fancies were inane and childish , and he believed it because he could see that they might easily be so .\n",
      "HPL\n",
      "\n",
      "\n",
      "After the ceremony was performed a large party assembled at my father ' s , but it was agreed that Elizabeth and I should commence our journey by water , sleeping that night at Evian and continuing our voyage on the following day .\n",
      "MWS\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = list(zip(train_x_tok, train_y_tok))\n",
    "\n",
    "for i, example in enumerate(train_set[:3]):\n",
    "    print('Example {}\\n{}\\n'.format(i,example))\n",
    "print()\n",
    "vocab.translate_examples(train_set[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set\n",
    "\n",
    "Do the same for the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/spooky_author_identification/processed/valid/sentences.txt ...\n",
      "Writing data/spooky_author_identification/processed/valid/ids_sentences.txt ...\n",
      "Writing data/spooky_author_identification/processed/valid/labels.txt ...\n",
      "Writing data/spooky_author_identification/processed/valid/ids_labels.txt ...\n",
      "Upon considerations similar to these , and still retaining my grasp upon the nose of Mr . W . , I accordingly thought proper to model my reply .\n",
      "EAP\n",
      "\n",
      "\n",
      "Most interesting of all was a glancing reference to the strange jewellery vaguely associated with Innsmouth .\n",
      "HPL\n",
      "\n",
      "\n",
      "The curtains , still clutched in his right hand as his left clawed out at me , grew _UNK and finally crashed down from their lofty fastenings ; admitting to the room a flood of that full moonlight which the _UNK of the sky had _UNK .\n",
      "HPL\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_x, valid_y = data_manager.prepare_valid()\n",
    "\n",
    "valid_x_tok = vocab.data_to_token_ids(valid_x, 'valid')\n",
    "valid_y_tok = vocab.labels_to_token_ids(valid_y, 'valid')\n",
    "\n",
    "valid_set = list(zip(valid_x_tok, valid_y_tok))\n",
    "\n",
    "vocab.translate_examples(valid_set[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching and Padding\n",
    "\n",
    "Batching is training with a set of examples at a time, rather than 1 example or the entire training set. This can improve the efficiency of the training. Since we intend to use an LSTM model, this has a set sequence length, so we need to pad the sequences in a batch to the same length.\n",
    "\n",
    "The below `Batches` class pads all the examples in the batch to the same length, and provides two generators. One generator which produces padded batches, and another that generates a certain number of Epochs, shuffling the data between each epoch.\n",
    "\n",
    "Later the training code will call\n",
    "\n",
    "`batches = Batches(batch_size)`\n",
    "\n",
    "to initialise the object with a certain batch size. Then it will call the generator\n",
    "\n",
    "`batches.gen_padded_batch_epochs(train_set, n_epochs)`\n",
    "\n",
    "which will step through a shuffled `train_set`, `n_epoch` times, where each step through the data has `batch_size` examples that are padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from random import shuffle\n",
    "\n",
    "class Batches(object):\n",
    "    def __init__(self, batch_size, pad_sym=0):\n",
    "        self.batch_size = batch_size\n",
    "        self.pad_sym = pad_sym\n",
    "\n",
    "    def pad_batch(self, batch):\n",
    "        max_len = max([len(b_seq) for b_seq in batch])\n",
    "        lengths = []\n",
    "\n",
    "        for idx, batched_seq in enumerate(batch):\n",
    "            current_len = len(batched_seq)\n",
    "            lengths.append(current_len)\n",
    "            if current_len < max_len:\n",
    "                padding = [self.pad_sym for _ in range(max_len - current_len)]\n",
    "                batched_seq.extend(padding)\n",
    "        return batch, lengths\n",
    "\n",
    "    def gen_padded_batches(self, data):\n",
    "        X, Y = zip(*data)\n",
    "        data_len = len(X)\n",
    "        n_steps = data_len // self.batch_size\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            idx_start = self.batch_size * step\n",
    "            idx_end   = self.batch_size * (step+1)\n",
    "            batch_x = X[ idx_start : idx_end ]\n",
    "            batch_y = Y[ idx_start : idx_end ]\n",
    "            padded_x, lengths = self.pad_batch(batch_x)\n",
    "            yield (padded_x, batch_y, lengths)\n",
    "\n",
    "    def gen_padded_batch_epochs(self, data, num_epochs):\n",
    "        for i in range(num_epochs):\n",
    "            shuffled_data = copy.deepcopy(data)\n",
    "            shuffle(shuffled_data)\n",
    "            yield self.gen_padded_batches(shuffled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LSTM/RNN Model\n",
    "\n",
    "The RNN model is defined as a class (`RNNModel`). In TensorFlow, switching RNN type can be as simple as switching which RNN Cell you use. So this class supports both LSTM cells and GRU cells. (This code is a bit dirty, as it was being updated between different versions of the TensorFlow API, but it works).\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "There are placeholders for a batch of tokenised sentences, `x`; the lengths of those sequences, `seqlen`; and the label IDs, `y`. It also takes in a placeholder for the dropout keep neuron output probability, so that this can be switched between training and test time.\n",
    "\n",
    "\n",
    "### Multiple Layers\n",
    "\n",
    "This class can also generate a multi-layered RNN. A function is defined, `cell_gen()`, that calls a constructor for an LSTM or GRU cell (function is redefined for GRU).\n",
    "Then if more layers are requested, a list of them are passed to the `MultiRNNCell` constructor.\n",
    "\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "The model also learns embeddings for the input token IDs, which are used with the LSTM/GRU.\n",
    "\n",
    "### Prediction\n",
    "\n",
    "The embeddings are used as input, along with the sequence lengths, to TensorFlows `dynamic_rnn` API. This will calculate a final vector the same size as the number of hidden units. A simple linear equation is used to translate this to the same number of outputs as there are classes. Finally, a softmax of the outputs is taken to get the class probabilities.\n",
    "\n",
    "### Loss and Optimisation\n",
    "\n",
    "For the loss function, Cross-Entropy is used, and the Adam optimiser calculates the weight updates.\n",
    "\n",
    "### Convenience Functions\n",
    "\n",
    "Some convenience functions are defined to create the model from some hyper-parameters (used later for tuning the architecture), for saving the model, and loading models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel(object):\n",
    "    def __init__(self, h_size, num_layers, vocab_size, n_classes, batch_size, rnn_type='lstm'):\n",
    "\n",
    "        # Input Placeholders\n",
    "        self.x = x = tf.placeholder(tf.int32, [batch_size, None], name=\"inputs\") # [batch_size, num_steps]\n",
    "        self.seqlen = seqlen = tf.placeholder(tf.int32, [batch_size], name=\"sequence_lengths\")\n",
    "        self.y = y = tf.placeholder(tf.int32, [batch_size], name=\"classes_gt\")\n",
    "        self.keep_prob = keep_prob = tf.placeholder(\"float\")\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        def cell_gen():\n",
    "            return tf.contrib.rnn.BasicLSTMCell(h_size, state_is_tuple=True)\n",
    "        if rnn_type == 'gru':\n",
    "            def cell_gen():\n",
    "                return tf.contrib.rnn.GRUCell(h_size)\n",
    "        \n",
    "        if num_layers > 1:\n",
    "            cells = []\n",
    "            for _ in range(num_layers):\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(cell_gen(), output_keep_prob=keep_prob)\n",
    "                cells.append(cell)\n",
    "        \n",
    "            cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        else:\n",
    "            cell = cell_gen()\n",
    "        \n",
    "        self.cell = cell\n",
    "        \n",
    "        # TODO: Prepare init state\n",
    "#         # Initialise one hidden state\n",
    "#         init_state = tf.get_variable('init_state', [1, h_size],\n",
    "#                                  initializer=tf.constant_initializer(0.0))\n",
    "#         # Tile to match batch_size\n",
    "#         init_state = tf.tile(init_state, [batch_size, 1])\n",
    "#         print(init_state)\n",
    "        \n",
    "        # Embedding layer\n",
    "        embeddings = tf.get_variable('embedding_matrix', [vocab_size, h_size])\n",
    "        rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "        \n",
    "#         rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "#                                                      initial_state=init_state)\n",
    "        rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen, dtype=tf.float32)\n",
    "\n",
    "        #idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "        #last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)        \n",
    "        last_rnn_output = tf.gather_nd(rnn_outputs, tf.stack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "        # Softmax layer\n",
    "        with tf.variable_scope('softmax'):\n",
    "            W = tf.get_variable('W', [h_size, n_classes])\n",
    "            b = tf.get_variable('b', [n_classes], initializer=tf.constant_initializer(0.0))\n",
    "        logits = tf.matmul(last_rnn_output, W) + b\n",
    "        preds = tf.nn.softmax(logits)\n",
    "        correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "        self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "        self.train_step = tf.train.AdamOptimizer(1e-4).minimize(self.loss, global_step=self.global_step)\n",
    "        \n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "        \n",
    "        self._prepare_logs()\n",
    "        \n",
    "    def _prepare_logs(self):\n",
    "        tf.summary.scalar('Loss', self.loss)\n",
    "        tf.summary.scalar('Accuracy', self.accuracy)\n",
    "        \n",
    "        self.logs = tf.summary.merge_all()\n",
    "\n",
    "def create_model(session, logdir, **parameters):\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        print('\\nCreating model with parameters:')\n",
    "        for k,v in parameters.items():\n",
    "            print('{:16s}: {}'.format(k, v))\n",
    "        model_train = RNNModel(parameters['h_size'], parameters['rnn_layers'], FLAGS_in_vocab_size,\n",
    "                               FLAGS_n_classes, parameters['batch_size'])\n",
    "        \n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    #print(ckpt.model_checkpoint_path)\n",
    "    if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path + '.index'):\n",
    "        print(\"Loading model from parameters in {}.\".format(ckpt.model_checkpoint_path))\n",
    "        model_train.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Creating model with fresh parameters.\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    return model_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code\n",
    "\n",
    "Training is carried out with the `train_net()` method below. It takes in the train and valid datasets, the number of epochs to run over the training set, as well as hyperparameters and an experiment run name.\n",
    "\n",
    "### Logging\n",
    "\n",
    "Two forms of logging are carried out. One which writes summaries for TensorBoard, allowing the training to be monitored, and quick checks of the output. Also, a `Logger` from `data_tools` is also used, to keep track of a few variables and write out a CSV later for more detailed plotting.\n",
    "\n",
    "### Epochs and Batching\n",
    "\n",
    "As mentioned earlier in the description of the Batching and Padding, the Batches object is used to generate the epochs and the padded batches for each epoch. The training loops come from these generators.\n",
    "\n",
    "### Iterations\n",
    "\n",
    "For each iteration, a `feed_dict` or feed dictionary is defined. This is a collection of inputs to feed into our computation graph. Here the input batch of token and label IDs are fed in, along with the sequence lengths, and the Dropout keep probability. The model object holds the references to the placeholders to be fed into.\n",
    "\n",
    "Then there is a `fetch`, or what we want out of the calculation graph. The main thing is the `train_step` which performs the forward and backward passes, but we also pull out the accuracy, loss, and logging summaries.\n",
    "\n",
    "### Validation\n",
    "\n",
    "At the end of the epoch, a validation check is performed. The details of this is in the `valid_eval()` method. Basically it loops through one epoch of the validation set, and sets the Dropout keep probability to 1.0. Then it reports on the average validation accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "FLAGS_in_vocab_size = 20000\n",
    "FLAGS_n_classes = 3\n",
    "FLAGS_log_dir = 'logs/'\n",
    "\n",
    "n_epochs = 1\n",
    "batch_size = 10\n",
    "n_steps_avg = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def valid_eval(sess, model, valid_set, batches):\n",
    "    total_steps = 0\n",
    "    val_accuracy = 0\n",
    "    val_loss = 0\n",
    "    for epoch in batches.gen_padded_batch_epochs(valid_set, 1):\n",
    "        for step, (batch_x, batch_y, lengths) in enumerate(epoch):\n",
    "            total_steps += 1\n",
    "            feed = {model.x: batch_x, model.y: batch_y, model.seqlen: lengths, model.keep_prob: 1.0}\n",
    "            fetch = [model.accuracy, model.loss]\n",
    "            \n",
    "            val_accuracy_, val_loss_ = sess.run(fetch, feed_dict=feed)\n",
    "            val_accuracy += val_accuracy_\n",
    "            val_loss += val_loss_\n",
    "    avg_val_accuracy = val_accuracy / total_steps\n",
    "    avg_val_loss = val_loss / total_steps\n",
    "    \n",
    "    return avg_val_accuracy, avg_val_loss\n",
    "\n",
    "\n",
    "\n",
    "def train_net(train_set, valid_set, n_epochs, run_name, **params):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        log_dir = os.path.join(FLAGS_log_dir, run_name)\n",
    "        log_txt_dir = os.path.join(FLAGS_log_dir, 'txtlogs/')\n",
    "        if not os.path.exists(log_txt_dir):\n",
    "            os.makedirs(log_txt_dir)\n",
    "        \n",
    "        model = create_model(sess, log_dir, **params)\n",
    "        \n",
    "        # Dropout keep neuron output probability\n",
    "        keep_prob = params['dropout_keep']\n",
    "        \n",
    "        batches = Batches(params['batch_size'])\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "        valid_writer = tf.summary.FileWriter(log_dir + '/valid', sess.graph)\n",
    "        \n",
    "        quantities = ['Gstep', 'Accuracy', 'Loss', 'Time']\n",
    "        train_logs = dt.Logger(*quantities)\n",
    "        valid_logs = dt.Logger(*quantities)\n",
    "\n",
    "        start_time = timer()\n",
    "\n",
    "        for i, epoch in enumerate(batches.gen_padded_batch_epochs(train_set, n_epochs)):\n",
    "            print('\\nEpoch', i+1)\n",
    "            accuracy = 0\n",
    "            loss = 0\n",
    "            for step, (batch_x, batch_y, lengths) in enumerate(epoch):\n",
    "\n",
    "                feed = {model.x: batch_x, model.y: batch_y, model.seqlen: lengths, model.keep_prob: keep_prob}\n",
    "                fetch = [model.accuracy, model.loss, model.logs, model.train_step]\n",
    "\n",
    "                accuracy_, loss_, logs, _ = sess.run(fetch, feed_dict=feed)\n",
    "                accuracy += accuracy_\n",
    "                loss += loss_\n",
    "\n",
    "                gstep = model.global_step.eval()\n",
    "                elapsed = timer() - start_time\n",
    "\n",
    "                train_writer.add_summary(logs, gstep)\n",
    "                train_logs.log(Gstep=gstep, Accuracy=accuracy_, Loss=loss_, Time=elapsed)\n",
    "\n",
    "                if step % n_steps_avg == 0 and step > 0:\n",
    "                    avg_accuracy = accuracy/n_steps_avg\n",
    "                    avg_loss = loss/n_steps_avg\n",
    "                    print('Step {:6d}, accuracy: {:7.3f}, loss: {:7.3f}, {:6.1f}s elapsed ({} steps avg.)'.format(\n",
    "                        gstep, avg_accuracy, avg_loss, elapsed, n_steps_avg))\n",
    "                    accuracy = 0\n",
    "                    loss = 0                          \n",
    "\n",
    "            valid_accuracy, valid_loss = valid_eval(sess, model, valid_set, batches)\n",
    "            print('Global Step {}, valid accuracy: {:7.3}'.format(gstep, valid_accuracy))\n",
    "            \n",
    "            elapsed = timer() - start_time\n",
    "            valid_logs.log(Gstep=gstep, Accuracy=valid_accuracy, Loss=valid_loss, Time=elapsed)\n",
    "\n",
    "            summary = tf.Summary()\n",
    "            summary.value.add(tag=\"model/Accuracy\", simple_value=valid_accuracy)\n",
    "            summary.value.add(tag=\"model/Loss\", simple_value=valid_loss)\n",
    "            valid_writer.add_summary(summary, gstep)\n",
    "            valid_writer.flush()\n",
    "\n",
    "            tf.logging.info('Step {} validation accuracy: {:7.3}'.format(gstep, valid_accuracy))\n",
    "\n",
    "            checkpoint_path = os.path.join(log_dir, 'crm_lstm.ckpt')\n",
    "            model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "\n",
    "        print('Done Training')\n",
    "\n",
    "        train_logs.write_csv(os.path.join(log_txt_dir, run_name + '_train.csv'))\n",
    "        valid_logs.write_csv(os.path.join(log_txt_dir, run_name + '_valid.csv'))\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuner (Grid Search)\n",
    "\n",
    "Here a simple parameter tuner using Grid Search is defined. It basically loops through each combination of the parameters. Here it deals with the following parameters:\n",
    "\n",
    "- h_sizes: The size of the hidden units in each RNN (same for all layers)\n",
    "- rnn_layers: How many RNNs to stack on top of each other\n",
    "- batch_sizes: How many examples to have in each mini-batch\n",
    "- dropout_keep: Probability of keeping the output of each RNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter sets\n",
    "import itertools\n",
    "\n",
    "class ParameterTuner(object):\n",
    "    def __init__(self):\n",
    "        self.h_sizes = None\n",
    "        self.rnn_layers = None\n",
    "        self.batch_sizes = None\n",
    "        self.dropout_keep = None\n",
    "    \n",
    "    def n_sets(self):\n",
    "        return len(self.h_sizes)*len(self.rnn_layers)*len(self.batch_sizes)*len(self.dropout_keep)\n",
    "    \n",
    "    def sets(self):\n",
    "        parameters = [self.h_sizes, self.rnn_layers, self.batch_sizes, self.dropout_keep]\n",
    "        for h, layers, batches, dropouts in itertools.product(*parameters):\n",
    "            par_set = {}\n",
    "            par_set['h_size'] = h\n",
    "            par_set['rnn_layers'] = layers\n",
    "            par_set['batch_size'] = batches\n",
    "            par_set['dropout_keep'] = dropouts\n",
    "            par_string = 'h{}_l{}_b{}_d{}'.format(h, layers, batches, dropouts)\n",
    "            yield par_set, par_string\n",
    "\n",
    "# h_sizes = [128, 256, 512, 1024]\n",
    "# rnn_layers = [1, 2, 3, 4]\n",
    "# batch_sizes = [16, 32, 64, 128]\n",
    "\n",
    "# h_sizes = [128, 256]\n",
    "# rnn_layers = [1, 2]\n",
    "# batch_sizes = [16, 32]\n",
    "\n",
    "h_sizes = [256]\n",
    "rnn_layers = [1]\n",
    "batch_sizes = [16]\n",
    "dropout_keep = [0.7]\n",
    "\n",
    "\n",
    "tuner = ParameterTuner()\n",
    "tuner.h_sizes = h_sizes\n",
    "tuner.rnn_layers = rnn_layers\n",
    "tuner.batch_sizes = batch_sizes\n",
    "tuner.dropout_keep = dropout_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "\n",
    "Here we do the hyper-parameter tuning, by looping through the combinations of parameters output by the `ParameterTuner`, which also outputs a string to identify each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Run 1/1,    0.0s elapsed\n",
      "\n",
      "Creating model with parameters:\n",
      "h_size          : 256\n",
      "rnn_layers      : 1\n",
      "batch_size      : 16\n",
      "dropout_keep    : 0.7\n",
      "Loading model from parameters in logs/h256_l1_b16_d0.7/crm_lstm.ckpt-1110.\n",
      "INFO:tensorflow:Restoring parameters from logs/h256_l1_b16_d0.7/crm_lstm.ckpt-1110\n",
      "\n",
      "Epoch 1\n",
      "Step   1161, accuracy:   0.522, loss:   1.029,   16.7s elapsed (50 steps avg.)\n",
      "Global Step 1210, valid accuracy:   0.559\n",
      "INFO:tensorflow:Step 1210 validation accuracy:   0.559\n",
      "Done Training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "epochs = 1\n",
    "\n",
    "tune_start = timer()\n",
    "n_psets = tuner.n_sets()\n",
    "for iset, (pset, pstring) in enumerate(tuner.sets()):\n",
    "    tune_elapsed = timer() - tune_start\n",
    "    print('\\n\\nRun {}/{}, {:6.1f}s elapsed'.format(iset+1, n_psets, tune_elapsed))\n",
    "    train_net(train_set[:1600], valid_set, epochs, pstring, **pset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
