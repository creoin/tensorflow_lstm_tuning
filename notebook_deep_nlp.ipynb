{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal setup for LSTM\n",
    "\n",
    "Currently set with Spooky Data, but will be switched to the Kaggle deep nlp dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# minimal\n",
    "import data_tools as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test data found, loading...\n",
      "Dataset prepared\n",
      "Building vocabulary\n",
      "  processing line 5000\n",
      "  processing line 10000\n",
      "  processing line 15000\n",
      "Writing data/spooky_author_identification/tmp2/vocab_sentences.txt ...\n",
      "Writing data/spooky_author_identification/tmp2/vocab_labels.txt ...\n",
      "  tokenising line 5000\n",
      "  tokenising line 10000\n",
      "  tokenising line 15000\n",
      "Writing data/spooky_author_identification/tmp2/train/sentences.txt ...\n",
      "Writing data/spooky_author_identification/tmp2/train/ids_sentences.txt ...\n",
      "Writing data/spooky_author_identification/tmp2/train/labels.txt ...\n",
      "Writing data/spooky_author_identification/tmp2/train/ids_labels.txt ...\n"
     ]
    }
   ],
   "source": [
    "# NLP Spooky Author Identification Dataset\n",
    "filepath = 'data/spooky_author_identification/train.csv'\n",
    "data_manager = dt.SpookyData(filepath, (0.8, 0.1, 0.1), one_hot_encode=False, output_numpy=False)\n",
    "data_manager.init_dataset()\n",
    "train_x, train_y = data_manager.prepare_train()\n",
    "\n",
    "# Vocabulary\n",
    "vocab = dt.Vocabulary('data/spooky_author_identification/tmp2', 20000)\n",
    "vocab.build_vocabulary(train_x, train_y)\n",
    "\n",
    "sents_vocab, rev_sents_vocab = vocab.get_sentence_vocabulary()\n",
    "label_vocab, rev_label_vocab = vocab.get_label_vocabulary()\n",
    "\n",
    "train_x_tok = vocab.data_to_token_ids(train_x, 'train')\n",
    "train_y_tok = vocab.labels_to_token_ids(train_y, 'train')\n",
    "\n",
    "train_set = list(zip(train_x_tok, train_y_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating token IDs back into words:\n",
      "\n",
      "It occurred to me that I must be in a highly nervous state to let a few random creakings set me off speculating in this fashion but I regretted none the less that I was unarmed .\n",
      "HPL\n",
      "\n",
      "\n",
      "Hence there is less distinction between the several classes of its inhabitants ; and the lower orders , being neither so poor nor so despised , their manners are more refined and moral .\n",
      "MWS\n",
      "\n",
      "\n",
      "M . St . Eustache , the lover and intended husband of Marie , who boarded in her mother ' s house , deposes that he did not hear of the discovery of the body of his intended until the next morning , when M . Beauvais came into his chamber and told him of it .\n",
      "EAP\n",
      "\n",
      "\n",
      "I had sufficient leisure for these and many other reflections during my journey to Ingolstadt , which was long and fatiguing .\n",
      "MWS\n",
      "\n",
      "\n",
      "Monsieur Le Blanc was unable to account for her absence , and Madame RogÃªt was distracted with anxiety and terror .\n",
      "EAP\n",
      "\n",
      "\n",
      "\n",
      "Actual form of training data:\n",
      " ([63, 772, 7, 26, 12, 8, 95, 33, 10, 9, 1580, 1374, 362, 7, 253, 9, 177, 7356, 19755, 349, 26, 219, 7213, 10, 37, 1328, 31, 8, 4556, 415, 3, 223, 12, 8, 11, 12997, 5], 1)\n"
     ]
    }
   ],
   "source": [
    "print('Translating token IDs back into words:\\n')\n",
    "vocab.translate_examples(train_set[:5])\n",
    "print('\\nActual form of training data:\\n', train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/spooky_author_identification/tmp2/valid/sentences.txt ...\n",
      "Writing data/spooky_author_identification/tmp2/valid/ids_sentences.txt ...\n",
      "Writing data/spooky_author_identification/tmp2/valid/labels.txt ...\n",
      "Writing data/spooky_author_identification/tmp2/valid/ids_labels.txt ...\n",
      "From my experience I cannot doubt but that man , when lost to terrestrial consciousness , is indeed _UNK in another and uncorporeal life of far different nature from the life we know ; and of which only the slightest and most indistinct memories linger after waking .\n",
      "HPL\n",
      "\n",
      "\n",
      "The device is that of a single individual ; and this brings us to the fact that ' between the thicket and the river , the rails of the _UNK were found taken down , and the ground bore evident traces of some heavy burden having been dragged along it ' But would a number of men have put themselves to the superfluous trouble of taking down a fence , for the purpose of dragging through it a corpse which they might have lifted over any fence in an instant ?\n",
      "EAP\n",
      "\n",
      "\n",
      "Whither he has gone , I do not know ; but I have gone home to the pure New England lanes up which fragrant sea winds sweep at evening .\n",
      "HPL\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare Validation Set\n",
    "valid_x, valid_y = data_manager.prepare_valid()\n",
    "\n",
    "valid_x_tok = vocab.data_to_token_ids(valid_x, 'valid')\n",
    "valid_y_tok = vocab.labels_to_token_ids(valid_y, 'valid')\n",
    "\n",
    "valid_set = list(zip(valid_x_tok, valid_y_tok))\n",
    "\n",
    "vocab.translate_examples(valid_set[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel(object):\n",
    "    def __init__(self, h_size, num_layers, vocab_size, n_classes, batch_size, dropout_keep=1.0, rnn_type='lstm'):\n",
    "\n",
    "        # Input Placeholders\n",
    "        self.x = x = tf.placeholder(tf.int32, [batch_size, None], name=\"inputs\") # [batch_size, num_steps]\n",
    "        self.seqlen = seqlen = tf.placeholder(tf.int32, [batch_size], name=\"sequence_lengths\")\n",
    "        self.y = y = tf.placeholder(tf.int32, [batch_size], name=\"classes_gt\")\n",
    "        keep_prob = tf.constant(dropout_keep)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        def cell_gen():\n",
    "            return tf.contrib.rnn.BasicLSTMCell(h_size, state_is_tuple=True)\n",
    "        if rnn_type == 'gru':\n",
    "            def cell_gen():\n",
    "                return tf.contrib.rnn.GRUCell(h_size)\n",
    "        \n",
    "        if num_layers > 1:\n",
    "            cells = []\n",
    "            for _ in range(num_layers):\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(cell_gen(), output_keep_prob=keep_prob)\n",
    "                cells.append(cell)\n",
    "        \n",
    "            cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        else:\n",
    "            cell = cell_gen()\n",
    "        \n",
    "        self.cell = cell\n",
    "        \n",
    "        # TODO: Prepare init state\n",
    "#         # Initialise one hidden state\n",
    "#         init_state = tf.get_variable('init_state', [1, h_size],\n",
    "#                                  initializer=tf.constant_initializer(0.0))\n",
    "#         # Tile to match batch_size\n",
    "#         init_state = tf.tile(init_state, [batch_size, 1])\n",
    "#         print(init_state)\n",
    "        \n",
    "        # Embedding layer\n",
    "        embeddings = tf.get_variable('embedding_matrix', [vocab_size, h_size])\n",
    "        rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "        \n",
    "#         rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "#                                                      initial_state=init_state)\n",
    "        rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen, dtype=tf.float32)\n",
    "\n",
    "        #idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "        #last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)        \n",
    "        last_rnn_output = tf.gather_nd(rnn_outputs, tf.stack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "        # Softmax layer\n",
    "        with tf.variable_scope('softmax'):\n",
    "            W = tf.get_variable('W', [h_size, n_classes])\n",
    "            b = tf.get_variable('b', [n_classes], initializer=tf.constant_initializer(0.0))\n",
    "        logits = tf.matmul(last_rnn_output, W) + b\n",
    "        preds = tf.nn.softmax(logits)\n",
    "        correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "        self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "        self.train_step = tf.train.AdamOptimizer(1e-4).minimize(self.loss, global_step=self.global_step)\n",
    "        \n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "        \n",
    "        self._prepare_logs()\n",
    "        \n",
    "    def _prepare_logs(self):\n",
    "        tf.summary.scalar('Loss', self.loss)\n",
    "        tf.summary.scalar('Accuracy', self.accuracy)\n",
    "        \n",
    "        self.logs = tf.summary.merge_all()\n",
    "\n",
    "def create_model(session, logdir, **parameters):\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        print('\\nCreating model with parameters:')\n",
    "        for k,v in parameters.items():\n",
    "            print('{:16s}: {}'.format(k, v))\n",
    "        model_train = RNNModel(parameters['h_size'], parameters['rnn_layers'], FLAGS_in_vocab_size,\n",
    "                               FLAGS_n_classes, parameters['batch_size'], dropout_keep=parameters['dropout_keep'])\n",
    "        \n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    #print(ckpt.model_checkpoint_path)\n",
    "    if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path + '.index'):\n",
    "        print(\"Loading model from parameters in {}.\".format(ckpt.model_checkpoint_path))\n",
    "        model_train.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Creating model with fresh parameters.\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    return model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FLAGS_in_vocab_size = 20000\n",
    "FLAGS_n_classes = 3\n",
    "FLAGS_log_dir = 'logs/'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "pars = {'h_size': 512, 'rnn_layers': 3, 'batch_size': 16, 'dropout_keep': 0.7}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = create_model(sess, FLAGS_log_dir, **pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "FLAGS_in_vocab_size = 20000\n",
    "FLAGS_n_classes = 3\n",
    "FLAGS_log_dir = 'logs/'\n",
    "\n",
    "n_epochs = 1\n",
    "batch_size = 10\n",
    "n_steps_avg = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def valid_eval(sess, model, valid_set, batches):\n",
    "    total_steps = 0\n",
    "    val_accuracy = 0\n",
    "    val_loss = 0\n",
    "    for epoch in batches.gen_padded_batch_epochs(valid_set, 1):\n",
    "        for step, (batch_x, batch_y, lengths) in enumerate(epoch):\n",
    "            total_steps += 1\n",
    "            feed = {model.x: batch_x, model.y: batch_y, model.seqlen: lengths}\n",
    "            fetch = [model.accuracy, model.loss]\n",
    "            \n",
    "            val_accuracy_, val_loss_ = sess.run(fetch, feed_dict=feed)\n",
    "            val_accuracy += val_accuracy_\n",
    "            val_loss += val_loss_\n",
    "    avg_val_accuracy = val_accuracy / total_steps\n",
    "    avg_val_loss = val_loss / total_steps\n",
    "    \n",
    "    return avg_val_accuracy, avg_val_loss\n",
    "\n",
    "\n",
    "\n",
    "def train_net(train_set, valid_set, n_epochs, run_name, **params):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        log_dir = os.path.join(FLAGS_log_dir, run_name)\n",
    "        log_txt_dir = os.path.join(FLAGS_log_dir, 'txtlogs/')\n",
    "        if not os.path.exists(log_txt_dir):\n",
    "            os.makedirs(log_txt_dir)\n",
    "        \n",
    "        model = create_model(sess, log_dir, **params)\n",
    "        \n",
    "        batches = dt.Batches(params['batch_size'])\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "        valid_writer = tf.summary.FileWriter(log_dir + '/valid', sess.graph)\n",
    "        \n",
    "        quantities = ['Gstep', 'Accuracy', 'Loss', 'Time']\n",
    "        train_logs = dt.Logger(*quantities)\n",
    "        valid_logs = dt.Logger(*quantities)\n",
    "\n",
    "        start_time = timer()\n",
    "\n",
    "        for i, epoch in enumerate(batches.gen_padded_batch_epochs(train_set, n_epochs)):\n",
    "            print('\\nEpoch', i+1)\n",
    "            accuracy = 0\n",
    "            loss = 0\n",
    "            for step, (batch_x, batch_y, lengths) in enumerate(epoch):\n",
    "\n",
    "                feed = {model.x: batch_x, model.y: batch_y, model.seqlen: lengths}\n",
    "                fetch = [model.accuracy, model.loss, model.logs, model.train_step]\n",
    "\n",
    "                accuracy_, loss_, logs, _ = sess.run(fetch, feed_dict=feed)\n",
    "                accuracy += accuracy_\n",
    "                loss += loss_\n",
    "\n",
    "                gstep = model.global_step.eval()\n",
    "                elapsed = timer() - start_time\n",
    "\n",
    "                train_writer.add_summary(logs, gstep)\n",
    "                train_logs.log(Gstep=gstep, Accuracy=accuracy_, Loss=loss_, Time=elapsed)\n",
    "\n",
    "                if step % n_steps_avg == 0 and step > 0:\n",
    "                    avg_accuracy = accuracy/n_steps_avg\n",
    "                    avg_loss = loss/n_steps_avg\n",
    "                    print('Step {}, accuracy: {:7.3}, loss: {:7.3} ({} steps avg.)'.format(\n",
    "                        gstep, avg_accuracy, avg_loss, n_steps_avg))\n",
    "                    accuracy = 0\n",
    "                    loss = 0                          \n",
    "\n",
    "            valid_accuracy, valid_loss = valid_eval(sess, model, valid_set, batches)\n",
    "            print('Global Step {}, valid accuracy: {:7.3}'.format(gstep, valid_accuracy))\n",
    "            \n",
    "            elapsed = timer() - start_time\n",
    "            valid_logs.log(Gstep=gstep, Accuracy=valid_accuracy, Loss=valid_loss, Time=elapsed)\n",
    "\n",
    "            summary = tf.Summary()\n",
    "            summary.value.add(tag=\"model/Accuracy\", simple_value=valid_accuracy)\n",
    "            summary.value.add(tag=\"model/Loss\", simple_value=valid_loss)\n",
    "            valid_writer.add_summary(summary, gstep)\n",
    "            valid_writer.flush()\n",
    "\n",
    "            tf.logging.info('Step {} validation accuracy: {:7.3}'.format(gstep, valid_accuracy))\n",
    "\n",
    "            checkpoint_path = os.path.join(log_dir, 'crm_lstm.ckpt')\n",
    "            model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "\n",
    "        print('Done Training')\n",
    "\n",
    "        train_logs.write_csv(os.path.join(log_txt_dir, run_name + '_train.csv'))\n",
    "        valid_logs.write_csv(os.path.join(log_txt_dir, run_name + '_valid.csv'))\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter sets\n",
    "import itertools\n",
    "\n",
    "class ParameterTuner(object):\n",
    "    def __init__(self):\n",
    "        self.h_sizes = None\n",
    "        self.rnn_layers = None\n",
    "        self.batch_sizes = None\n",
    "        self.dropout_keep = None\n",
    "    \n",
    "    def n_sets(self):\n",
    "        return len(self.h_sizes)*len(self.rnn_layers)*len(self.batch_sizes)*len(self.dropout_keep)\n",
    "    \n",
    "    def sets(self):\n",
    "        parameters = [self.h_sizes, self.rnn_layers, self.batch_sizes, self.dropout_keep]\n",
    "        for h, layers, batches, dropouts in itertools.product(*parameters):\n",
    "            par_set = {}\n",
    "            par_set['h_size'] = h\n",
    "            par_set['rnn_layers'] = layers\n",
    "            par_set['batch_size'] = batches\n",
    "            par_set['dropout_keep'] = dropouts\n",
    "            par_string = 'h{}_l{}_b{}_d{}'.format(h, layers, batches, dropouts)\n",
    "            yield par_set, par_string\n",
    "\n",
    "# h_sizes = [128, 256, 512, 1024]\n",
    "# rnn_layers = [1, 2, 3, 4]\n",
    "# batch_sizes = [16, 32, 64, 128]\n",
    "\n",
    "# h_sizes = [128, 256]\n",
    "# rnn_layers = [1, 2]\n",
    "# batch_sizes = [16, 32]\n",
    "\n",
    "h_sizes = [256]\n",
    "rnn_layers = [1]\n",
    "batch_sizes = [16]\n",
    "dropout_keep = [0.7]\n",
    "\n",
    "\n",
    "tuner = ParameterTuner()\n",
    "tuner.h_sizes = h_sizes\n",
    "tuner.rnn_layers = rnn_layers\n",
    "tuner.batch_sizes = batch_sizes\n",
    "tuner.dropout_keep = dropout_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Run 1/1, 0.00013098897761665285s elapsed\n",
      "\n",
      "Creating model with parameters:\n",
      "batch_size      : 16\n",
      "rnn_layers      : 1\n",
      "dropout_keep    : 0.7\n",
      "h_size          : 256\n",
      "Loading model from parameters in logs/h256_l1_b16_d0.7/crm_lstm.ckpt-1010.\n",
      "INFO:tensorflow:Restoring parameters from logs/h256_l1_b16_d0.7/crm_lstm.ckpt-1010\n",
      "\n",
      "Epoch 1\n",
      "Step 1061, accuracy:   0.631, loss:   0.886 (50 steps avg.)\n",
      "Global Step 1110, valid accuracy:   0.654\n",
      "INFO:tensorflow:Step 1110 validation accuracy:   0.654\n",
      "Done Training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from data_tools import Logger\n",
    "epochs = 1\n",
    "\n",
    "tune_start = timer()\n",
    "n_psets = tuner.n_sets()\n",
    "for iset, (pset, pstring) in enumerate(tuner.sets()):\n",
    "    tune_elapsed = timer() - tune_start\n",
    "    print('\\n\\nRun {}/{}, {}s elapsed'.format(iset+1, n_psets, tune_elapsed))\n",
    "    train_net(train_set[:1600], valid_set, epochs, pstring, **pset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test Batches class\n",
    "\n",
    "n_epochs = 2\n",
    "for i, epoch in enumerate(batches.gen_padded_batch_epochs(train_set, n_epochs)):\n",
    "    print('\\n\\nEpoch {}'.format(i))\n",
    "    for step, (batch_x, batch_y, lengths) in enumerate(epoch):\n",
    "        print('{:5d} {} {}\\n\\n'.format(step, batch_x[:2], batch_y[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
