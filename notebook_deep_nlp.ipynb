{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Tuning for Kaggle Deep NLP Datasets\n",
    "\n",
    "In this notebook, an LSTM is tuned for the Deep NLP datasets on Kaggle. There is a more detailed notebook for the LSTM tuning for the Spooky Author Identification. In this notebook, the previous code will be reused with less explanation on two new datasets.\n",
    "\n",
    "### Deep NLP Dataset\n",
    "\n",
    "The Deep NLP Dataset contains two datasets from two different cases. One is responses to a chatbot, and one is resumes. Both of these have a label as `flagged` or `not flagged` (binary label).\n",
    "\n",
    "\n",
    "### Chatbot Dataset\n",
    "\n",
    "For the Chatbot dataset, the scenario is a therapy chatbot where the user is asked 'Describe a time when you have acted as a resource for someone else'. The dataset consists of user responses to this question, as well as a 'flagged' or 'not flagged' label. If it is 'flagged', the user is referred to help.\n",
    "\n",
    "The dataset is contained in Sheet_1.csv, and has 80 user responses in the response_text column.\n",
    "\n",
    "\n",
    "### Resume Dataset\n",
    "\n",
    "For the Resume dataset, resumes were queried from Indeed.com with keyword 'data scientist', location 'Vermont'. The dataset consists of the text from these resumes, as well as a 'flagged' or 'not flagged' label. In this case, 'flagged' means the applicant is invited to an interview, and 'not flagged' for when the user can submit a modified resume at a later date.\n",
    "\n",
    "The dataset is contained in Sheet_2.csv, and has 125 text resumes in the resume_text column.\n",
    "\n",
    "\n",
    "### Small Datasets\n",
    "\n",
    "Even though this dataset is called Deep NLP, these datasets have so few examples, that deep learning may not work so well. The validation sets we can build will also have a few examples, so it will also be difficult to assess the capailbity of the model to generalise and its expected accuracy. Perhaps in this case, you would want to use some less data hungry method, and engineer very specific features for these tasks to get a higher accuracy. However, such models will of course not generalise well to new cases.\n",
    "\n",
    "One goal of these datasets is to build one model that works for both cases, so the model choice/engineering should not be so specific to one case. Deep Learning models generalise much better, as they learn the relevant features from the datset. Anyway, let's try to use the LSTM model tuned with the Spooky Author Indentification dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Adapters for Deep NLP Dataset\n",
    "\n",
    "Firstly, make a simple CSV reader and taken a look at the format of the data. The Chatbot dataset reads fine, but the Resume dataset has some encoding problems. Most of the characters can be recognised by utf-8, while a few characters have strange encoding. These tend to be bullet points. So for the Resume case, `replace` these characters and do a little pre-processing afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import csv\n",
    "\n",
    "datadir = 'data/deepnlp/'\n",
    "csv_chatbot = os.path.join(datadir, 'sheet_1.csv')\n",
    "csv_resume = os.path.join(datadir, 'sheet_2.csv')\n",
    "filesdir = 'files/'\n",
    "\n",
    "# Read in CSV data\n",
    "def read_csv(filepath, file_encoding='utf-8', for_errors='strict', discard_header=False):\n",
    "    data_raw = []\n",
    "    with open(filepath, 'r', encoding=file_encoding, errors=for_errors) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            if not row:\n",
    "                continue\n",
    "            if discard_header and i == 0:\n",
    "                continue\n",
    "            data_raw.append(row)\n",
    "    return data_raw\n",
    "\n",
    "data_chatbot_raw = read_csv(csv_chatbot)\n",
    "data_resume_raw  = read_csv(csv_resume, for_errors='replace')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Look at Chatbot Data\n",
    "\n",
    "Taking a look at the first 10 entries of the therapy chatbot user response dataset (sheet_1.csv):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['response_id', 'class', 'response_text', '', '', '', '', '']\n",
      "\n",
      "['response_1', 'not_flagged', 'I try and avoid this sort of conflict', '', '', '', '', '']\n",
      "\n",
      "['response_2', 'flagged', 'Had a friend open up to me about his mental addiction to weed and how it was taking over his life and making him depressed', '', '', '', '', '']\n",
      "\n",
      "['response_3', 'flagged', 'I saved a girl from suicide once. She was going to swallow a bunch of pills and I talked her out of it in a very calm, loving way.', '', '', '', '', '']\n",
      "\n",
      "['response_4', 'not_flagged', 'i cant think of one really...i think i may have indirectly', '', '', '', '', '']\n",
      "\n",
      "['response_5', 'not_flagged', 'Only really one friend who doesn\\'t fit into the any of the above categories. Her therapist calls it spiraling.\" Anyway she pretty much calls me any time she is frustrated by something with  her boyfriend to ask me if it\\'s logical or not. Before they would just fight and he would call her crazy. Now she asks me if it\\'s ok he didn\\'t say \"please\" when he said  \"hand me the remote.\"', ' ', '', '', '', '']\n",
      "\n",
      "['response_6', 'not_flagged', 'a couple of years ago my friends was going to switch school because of low self esteem too. I helped him overcome that shit too', '', '', '', '', '']\n",
      "\n",
      "['response_7', 'flagged', 'Roommate when he was going through death and loss of a gf. Did anything to get him out of his bedroom.', '', '', '', '', '']\n",
      "\n",
      "['response_8', 'flagged', \"i've had a couple of friends (you could say more than friends) with quite severe depression/ emotional problems. i helped for a while but eventually both relationships started to suffer as a result of both our personal problems\", '', '', '', '', '']\n",
      "\n",
      "['response_9', 'not_flagged', 'Listened to someone talk about relationship troubles. Offered some advice from personal experience.', '', '', '', '', '']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in data_chatbot_raw[:10]:\n",
    "    print('{}\\n'.format(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "From the above, it looks like 'flagged' responses are ones which the user helped someone in a more severe case. The first row is a header row, and each row contains a response ID, the flagged/not_flagged class, and response text. There appears to be 5 empty columns after that. Also, apostrophes appear to have been escaped, but this looks like how Python shows strings with both double and single quotes. The escaping doesn't show when printing, and even if you use `\"\"\"` or `'''`, the value will still show the single quotes escaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Only really one friend who doesn\\'t fit into the any of the above categories. Her therapist calls it spiraling.\" Anyway she pretty much calls me any time she is frustrated by something with  her boyfriend to ask me if it\\'s logical or not. Before they would just fight and he would call her crazy. Now she asks me if it\\'s ok he didn\\'t say \"please\" when he said  \"hand me the remote.\"'\n"
     ]
    }
   ],
   "source": [
    "print(repr(data_chatbot_raw[5][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only really one friend who doesn't fit into the any of the above categories. Her therapist calls it spiraling.\" Anyway she pretty much calls me any time she is frustrated by something with  her boyfriend to ask me if it's logical or not. Before they would just fight and he would call her crazy. Now she asks me if it's ok he didn't say \"please\" when he said  \"hand me the remote.\"\n"
     ]
    }
   ],
   "source": [
    "print(data_chatbot_raw[5][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Look at Resume Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resume_id', 'class', 'resume_text']\n",
      "\n",
      "['resume_1', 'not_flagged', '\\nCustomer Service Supervisor/Tier - Isabella Catalog Company\\nSouth Burlington VT - Email me on Indeed: indeed.com/r//49f8c9aecf490d26\\nWORK EXPERIENCE\\nCustomer Service Supervisor/Tier\\nIsabella Catalog Company - Shelburne VT - August 2015 to Present\\n2 Customer Service/Visual Set Up & Display/Website Maintenance\\n��� Supervise customer service team of a popular catalog company\\n��� Manage day to day issues and resolution of customer upset to ensure customer satisfaction\\n��� Troubleshoot order and shipping issues: lost in transit order errors damages\\n��� Manage and resolve escalated customer calls to ensure customer satisfaction\\n��� Assist customers with order placing cross-selling/upselling of catalog merchandise\\n��� Set up and display of sample merchandise in catalog library as well as customer pick-up area of the facility ��� Website clean-up: adding images type up product information proofreading\\nAdministrative Assistant /Events Coordinator/Office Services Assistant\\nEileen Fisher Inc - Irvington NY - February 2014 to July 2015\\nSupport to Director of Architecture and Architecture Coordinator in all daily activities including: preparing monthly expense reports scheduling calendar maintenance arranging all aspects of travel/logistics catering interior design research projects\\n��� Manage event set ups through entire process for two Eileen Fisher corporate locations\\n��� Catering overseeing set up walk-thru of space with client review event forms with facilities team ��� Daily management of two professional calendars that require heavy scheduling\\n��� Office services that include: companywide room reservations office supply orders\\n��� Filtered calls to the Chief Creative Officer/Owner of the company\\nTemp Assignment\\nOrthoNet - White Plains NY - December 2013 to February 2014\\nOffice Services Assistant/Receptionist\\n��� Managed heavy call volume for orthopedic specialty benefit management company\\n��� Directed heavy daily incoming mail flow\\n��� Processed daily checks and entered data into Excel to generate totals for accounting reports\\nExecutive Personal Assistant\\nWestchester NY - January 2012 to December 2013\\nHome Office Assistant/ Personal Assistant\\n��� Provided professional office support to three established Psychologists in the New York area ��� Carefully handled personal and confidential patient information\\n��� Organized uncluttered and simplified office space to create a more user-friendly atmosphere ��� Coordinated and researched all travel related details (flights hotels visas cars etc.)\\n��� Managed personal errands phone calls and emails.\\n��� Responsible for mail processing and bank deposits while Psychologists were traveling\\nCustomer Service Representative/ Account Manager\\nCM Almy & Sons Inc - Greenwich CT - January 2007 to January 2012\\n \\nGreenwich CT January 2007 - January 2012\\nCustomer Service Representative/ Account Manager\\n��� Provided a high level of customer service to clergy and church members of all denominations\\n��� Answered heavy call volume and assisted customers in a highly efficient manor\\n��� Assisted customers with overall design of garments final decision making of church item purchases ��� Managed and maintained a large account database with daily phone calls to customer accounts\\n��� Responsible for tracking large shipments and also replacement of lost or damaged items\\n435 Dorset Street * South Burlington VT 05403 ��_ 914.564.4381 ��_ Aimeerblair319@gmail.com\\nAdministrative Assistant to Chief Financial Officer\\nCoalition to Salute Americas Heroes - Ossining NY - January 2005 to January 2007\\nOssining NY January 2005 - January 2007\\nAdministrative Assistant to Chief Financial Officer\\n��� Interviewed military veterans and their families to be considered for financial aid ��� Reviewed a highly confidential database for candidate\\n��� Mediated discussions between military veterans and collectors\\n��� Arrange final payouts for debt incurred during time of injury\\n��� Finalized paperwork for award payouts\\n��� Coordinated travel and logistics for large sponsored events\\n��� Assisted disabled veterans during events\\n��� Provided basic administrative support\\nAdministrative Assistant to Sales Team/ Trade Show Coordinator\\nLeo Electron Microscopy - Thornwood NY - May 2000 to August 2003\\nThornwood NY May 2000 - August 2003\\nAdministrative Assistant to Sales Team/ Trade Show Coordinator\\n��� Communicated general information and provided quotes to high end buyers\\n��� Worked closely with a team of sales associates arranging meetings with potential buyers\\n��� Prepared final proposals and closing sale information on purchased electron microscopes\\n��� Arranged all aspects of travel and logistics for trade shows within the United States and Canada.\\n��� Attended trade shows with sales associates and scientists to insure all electron microscopes arrived safely for set up\\n��� Assisted with demonstrations and close of sales on trade show floor\\nArtist Charles Fazzino 3D Pop Artist\\nCharles Fazzino - New Rochelle NY - 1993 to 1996\\nand 2003-2005\\nFreelance Artist\\n��� Assembled 3 dimensional piece-art on a weekly basis from home office\\n��� Responsible for detailed finishing work and making pieces presentable for purchase in galleries world-wide\\nEDUCATION\\nAAS in Visual Arts\\nWestchester Community College - New York NY School knowledge\\nADDITIONAL INFORMATION\\nProviding more than 15 years of combined office services with a focus on Administrative Assistance Customer Service Event Coordination Trade Show Coordination and Facilitating\\n']\n",
      "\n",
      "['resume_2', 'not_flagged', '\\nEngineer / Scientist - IBM Microelectronics Division\\nWestford VT - Email me on Indeed: indeed.com/r/Albert-Gregoritsch/b105a8b2b40f9eca\\nWORK EXPERIENCE\\nEngineer / Scientist\\nIBM Microelectronics Division - June 2007 to Present\\nResponsible for Process and Equipment engineering for multiple lines including: o Multiple Bake processes\\no Leaded and lead free solder reflow\\no Thermal cycling\\n��� Wrote specifications and procured capital for equipment purchases and upgrades ��� Project management for equipment installation and equipment upgrades\\n��� Developed methods for acquiring and tracking critical data metrics\\n��� Drove production efficiency gains through data-driven decision making\\n��� Implemented and maintained Lean Manufacturing initiatives o Root cause analysis and Structured Problem solving\\no Continuous Improvement activities\\no Standard Work and Job Breakdown Sheets\\n��� Utilized statistical process controls on critical process indicators\\n��� Worked with cross functional teams to implement and communicate changes\\n��� Improved process flow and reduced cycle time through waste elimination\\n��� Identified opportunities for and drove implementation of technical improvements to current manufacturing processes and procedures\\n��� Oversight and implementation of multiple complex projects simultaneously\\n��� Responsible for quality inspection strategy including inspection methods sample plans and defect criteria (June 2007 - July 2008)\\n��� Oversight of materials transport processes (June 2007 - July 2008)\\nEngineering Technician\\nIBM Microelectronics Division - June 2007 to June 2007\\nJune 2007)\\n��� Tooling and Process support for:\\no Metal mask cleaning and inspection processes o Multiple Bake processes\\no Leaded and lead free solder reflow\\no Thermal cycling\\n��� Documented manufacturing processes\\n��� Trained production operators\\n��� Qualified new equipment and processes\\nProduction Operator\\nIBM Microelectronics Division - May 1995 to May 1998\\nSupplemental Operator\\nIBM Microelectronics Division - September 1994 to May 1995\\n \\nSupervisor\\nEndicott Contract Manufacturing - October 1993 to September 1994\\nSupervised 19 employees working in quality control\\n��� Handled personnel issues including scheduling vacation planning hiring firing performance evaluations and resource actions\\nMicroscope Inspector\\nEndicott Contract Manufacturing - March 1993 to October 1993\\nSupplemental Operator\\nIBM - February 1991 to October 1992\\nEDUCATION\\nBachelors in Business Technology and Management\\nVermont Technical College\\nAssociates in General Engineering Technologies\\nVermont Technical College\\nADDITIONAL INFORMATION Skills\\n��� Microsoft Office Suite Lotus Symphony Windows AIX familiar with QMF SQL other database software and Microsoft Project\\n��� PLC controllers Fluke Dataloggers Omega Dataloggers Laser Particle Counters Microscopes Automated Inspection Equipment Belt furnaces Blue M ovens Thermal Cycling chambers\\n��� Lean Manufacturing initiatives Structured Problem solving 5S continuous improvement value stream analysis standard work visual pull systems\\n��� Excellent presentation skills public speaking power point\\n']\n",
      "\n",
      "['resume_3', 'not_flagged', \"\\nLTS Software Engineer Computational Lithography - IBM Corporation\\nBolton VT - Email me on Indeed: indeed.com/r/Albert-Nemethy/a21b1e4ebf733793\\nIn 1981 started as an equipment technician responsible for maintaining IBM's proprietary device test systems took an instant liking to developing tester applications written in basic. I a steady and productive climb into equipment engineering developing many software systems and applications along the way. From 87C52 device microcode for high speed servo applications to multi-process multi-threaded desktop graphical applications I have to hone my skills building industrial strength software for the global market. My most recent project involved developing blob analysis applications for wafer map processes using MS Visual C#. I continue to development my own products maintaining my software skills using the latest leading edge technology.\\nWORK EXPERIENCE\\nLTS Software Engineer Computational Lithography\\nIBM Corporation - September 2011 to Present\\nRe-engineering of unit test processes that guarentee functionality of all Perl based applications used for IBM's Computational Lithography Processes. The goal of this work is to automatically generate test files which will verify that all applications are coded to their functional specifications.\\nContractor for IBM. Software Engineer\\nCTG INC - September 2010 to Present\\nComputational Lithography\\nDeveloper of Perl based applications that preprocess mask data for IBM's Computational Lithography Processes. These applications incorporate the use of Mentor Graphics - Calibre - products to provide Optical Proximity Correction for IBM's 200 - 300mm wafer fab mask operations.\\nCustomer Engineer\\nAPPLIED MATERIALS INC - February 2007 to March 2010\\nDeveloped C# based application to graphically analyze wafer maps in an attempt to isolate hotspots on 300 mm wafers which would allow repositioning for optimum process operation. These programs used blob analysis to identify key areas of the wafer that indicated xy corrections direction and theta rotation.\\nMaterials Coordinator\\nNSTAR GLOBAL SERVICES - April 2004 to February 2007\\nMaterials coordinator. Handled spare parts for Applied Materials operations in Fishkill NY\\nSoftware Engineer\\nIBM Corporation - Burlington VT - May 1997 to September 2003\\nDeveloped a real time multithreaded Java based SAS command processor for wafer final test use. Using AWT I built a multi-window real time based Process scheduler that was run on an AIX platform. All code was in Java. FailData Analysis System. I Developed logic diagnostic applications for semiconductor device support at wafer final test. The purpose of one of these applications is to automatically pinpoint final test failures on all chips tested. The AIX/C/Motif based application FAST (Faildata Analysis SysTem) was an integral part. Version 2.x was developed using Java and swing for the GUI.\\nIBM Corporation - Poughkeepsie NY - March 1981 to September 2003\\n \\nSoftware Engineer/Scientist\\nIBM Corporation - Boca Raton FL - April 1992 to May 1997\\nHigh Speed Serial Prober. Tester interfaces. Built software interfaces for measurement devices like:SMU DMM Capacitance meter etc. Developed code in C to interface a 40 inch 8 axis Anorad gantry to drive our device measurement systems. Developed diagnostic code for all systems as well.\\nBoard Display Program. I Developed the Board Display Program. This was a 20000 line program written in C using OS2's presentation engine which allowed the user to graphically display the part under test and manipulate it to produce alignment and height mapping points. It also allowed an off-line method of optimizing a test sequence.\\nVisual Test Generator. I invented an offline OS/2 graphical application to generate a test file from the device's net list called the Visual Test Generator. This required many optimization algorithms and enabled the test engineer to graphically refine a test program for maximum optimization.\\nProgramming Technician\\nIBM Corporation - July 1983 to June 1986\\nPC Chip In Place Test Automation. Our team transformed an existing tester into a fully automatic functional chip tester. Utilizing ARTIC technology and Intel 87C51 embedded microcontrollers we developed multitasking real time software to control various pieces of the tester using C and PLM.\\nCIPT Automatic Alignment. Automatic alignment of test probe to pads was accomplished using Cognex machine vision and custom application code that we developed using C. This project also required us to develop a pre Windows GUI screen management system.\\nModule Assembly Tool Diagnostics. Developed maintenance diagnostic programs for automatic second generation module assembly tool. This was an Instron based servo driven compression tool that was used to compress the various pieces of a TCM together as it was being built. This was the first use of the C programming language at this site.\\nEquipment Maintenance Technician\\nIBM Corporation - March 1981 to July 1983\\nResponsible for repair of all LT-1280 and LT-128 Test equipment.\\nANN v2.3 01202013\\nEDUCATION\\nBachelor of Science in Electrical Engineering\\nKennedy Western University\\nEngineering\\nState University of NY\\nAssociates of Applied Science in Electrical Technology\\nState University of NY\\nLINKS http://users.gmavt.net/anemethy\\nADDITIONAL INFORMATION\\nTECHNICAL SKILLS:\\nEnvironments: MS Windows/Office AIX/Unix/Linux MSVisual C++ MSVisual C# Languages: C/C++/C# Java JavaScript Perl HTML XML Basic and Assembler Development Tools: Orcad Visual Age VC++ VC#\\nHardware: Motion control Anorad Kensington Ormac\\nMeasurement: Keithly HP NI Data Acquisition. IEEE Equipment Microcontrollers: Atmel 328 Arduino 80x51 family of\\nSystem Controllers: IBM HP Industrial PCs\\nTest Systems: Teradyne [...] Advantest 66xx\\n \"]\n",
      "\n",
      "['resume_4', 'not_flagged', \" TUTOR\\nWilliston VT - Email me on Indeed: indeed.com/r/Alec-Schwartz/7177c11327372c0a\\nWORK EXPERIENCE\\nTutor\\nDickinson College Biology Department - Carlisle PA - March 2016 to May 2016\\nI was invited to tutor three students enrolled in Biology 120: Life at the Extremes. I helped them learn as independently as possible while still acting as a mentor and guide.\\nTeaching Assistant\\nDickinson College Biology Department - Carlisle PA - January 2016 to May 2016\\nTaught by Professor Scott Boback this comparative physiology course explored how extremophiles are capable of surviving and maintaining\\nhomeostasis in harsh environments. I helped students perform hypothesis-driven physiology experiments and vertebrate dissections.\\nQA/QC Laboratory Coordinator\\nAlliance for Aquatic Resource Monitoring (ALLARM) - Carlisle PA - August 2015 to May 2016\\nALLARM a small NGO housed at Dickinson college engages communities to use science as a tool to investigate the health of their streams. I helped\\nmentor organizations to teach them to use the data they generate for aquatic protection and restoration efforts. I mainly worked in the lab performing\\nchemical tests for quality assurance verification. I also currently research and test equipment to verify precision accuracy and accessibility for citizen\\nscientists as well as conducting bimonthly baseline analysis of our local Retort Spring Run.\\nFirst-Year Mentor Wilderness Introduction to Life at Dickinson (WILD) Leader\\nCampus Life at Dickinson College - Carlisle PA - August 2013 to May 2016\\nThe First-Year Mentor program provides an opportunity for upper-level students to help new students experience a positive transition to college. I met with my First-Year Interest Group weekly to check in on their personal social and intellectual development at Dickinson. Last Fall I led a 3-day\\nbackpacking trip through a section of the Delaware Water Gap during orientation. After one of my first-year's suddenly passed away in the Fall '15 I co- led a partnership between Dickinson and our local YMCA's Safety Around Water Program to provide help during outreach enrollment and swim lessons. It has given our community a meaningful way to commemorate the life of Jigme Nidup.\\nCustomer Experience Intern\\nNaviNet - Boston MA - June 2015 to August 2015\\nNaviNet is America's leading healthcare collaboration network connecting over 40 health plans and 60% of the nation's physicians. I worked as a\\nCustomer Experience Intern drafting user-experience reports and presentations for NaviNet and health insurance plans to showcase how health providers were using different software products.\\nStudent Researcher\\nDickinson College Chemistry Department - Carlisle PA - March 2015 to May 2015\\n \\nProfessor Mike Holden's interests are in the area of organotransitionmetal-mediated synthesis of organic compounds. In his lab I worked to synthesize\\nknown biologically active compounds with the incorporation of ferrocene (an iron compound) to potentially boost efficacy of antimalarials.\\nStudent Researcher\\nBen-Gurion University of the Negev Jacob Blaustein Institute for Desert Research - Midreshet Israel - November 2014 to January 2015\\nSpecies adapted to fasting sequentially oxidize fuels in three discrete phases. Professor Berry Pinshow's lab is exploring the physiological\\nresponses to food deprivation in House and Spanish sparrows (birds not adapted to prolonged fasting). For two months I helped prepare dietary isotopic tracers run 32 hour-long experiments requiring hourly measurements for stable isotope signatures in real-time using a 13C-infrared analyzer (HeliFANplus) and interpret collected data using repeated measures (RM)-ANOVA.\\nPaper: Khalilieh A McCue MD Pinshow B. Physiological responses to food deprivation in the house sparrow a species not adapted to prolonged fasting. Am J Physiol Regul Integr Comp Physiol. 303: R551-R561 2012.\\nEDUCATION\\nBS in Biochemistry and Molecular Biology\\nDickinson College - Carlisle PA 2012 to 2016\\nSKILLS\\nNMR LCMS HPLC qPCR Gel Electrophoresis Western Blot Transfection various microscopy techniques (DIC Phase Contrast Brightfield Darkfield Fluorescent) recombinant DNA Plasmid Cloning Pymol CRISPR Cas9\\nLINKS http://blogs.dickinson.edu/writingsciencenewssp14/author/schwaral\\nAWARDS\\nCum Laude for BS in Biochemistry and Molecular Biology\\nMay 2016\\nMaintained an overall GPA of >3.50 throughout my time at Dickinson College.\\n \"]\n",
      "\n",
      "['resume_5', 'flagged', '\\nIndependent Consultant - Self-employed\\nBurlington VT - Email me on Indeed: indeed.com/r/Alex-Reutter/2c4a904a891a6fef\\nWORK EXPERIENCE\\nIndependent Consultant\\nSelf-employed - Burlington VT - October 2016 to Present Projects in progress.\\nSenior Data Scientist\\nIBM - 2015 to 2016\\nDeveloped product strategies for Data Science Experience (datascience.ibm.com) for machine learning algorithms and end-to- end usage for data scientists. Patient Zero for ensuring product design matches typical data scientist workflows.\\n��� Enabled connection with tens-of-thousands of customers onsite and in social media impressions by crafting story of data\\nscientist in auto industry and how use of SPSS and Spark evolved from analysis of spreadsheet data on defect rates of auto\\nparts to integration of PySpark into analytic work streams. Story was featured at 2015 Spark Signature Moment in IBM\\nSpeaker Presenter\\nIBM - 2013 to 2015\\nand in customer-facing videos and presentations.\\n��� Increased sales by evangelizing IBM analytics solutions to hundreds of customers onsite as Speaker Presenter at IBM\\nInsight 2013 - 2015.\\n��� Improved development of new product features including Data Science Experience (DSX) Watson Analytics SPSS\\nAdvisory Software Engineer (Data Scientist)\\nIBM - 2009 to 2015\\nLed team as Agile product owner for Analytic Server working with offering management and internal and external clients to coordinate and prioritize the development of new software features.\\n��� Facilitated quarterly (rather than yearly) release cycles and single integrated system for project tracking by transitioning\\n100+ developers from Waterfalls to Agile development processes as project focal.\\n��� Aided transformation of integrated supply chain (ISC) code base from R to more-easily maintainable SPSS by mentoring\\ncolleague on SPSS products from model prototyping to advanced forecasting techniques. Success story of collaboration between software group and ISC was presented to IBM analytics and inspired further internal partnerships.\\n��� Delivered Analytic Server 1.0 - 2.1 product by keeping development focused on important stories designing and implementing scripts to track sprint progress and analyze product backlog culling dead work items and celebrating\\nsuccesses.\\n \\nR. ALEXANDER (ALEX) REUTTER alex.reutter@gmail.com\\nMaster Statistical Writer\\nSPSS INC - 2006 to 2009\\nAuthored and contributed to designs integration standards and user-facing documentation of common statistical components across SPSS products.\\n��� Gave customers access to item response models otherwise unavailable in SPSS Statistics product by creating custom\\ncommands and dialogs using open source Python and R programmability.\\n��� Ensured quality and consistency of thousands of pages of documentation training and sales and marketing presentations by writing cross-product standards for developing examples and sample data that prove product functionality.\\n��� Reduced time to create command syntax documentation 80% and ensured designs / documentation were in sync by writing standards for single-sourcing command syntax designs and user-facing documentation.\\nSenior Statistical Writer\\nSPSS INC - 1998 to 2006\\nGave customers access to 1K+ pages of algorithms documentation in easily accessible online Help and PDF format by leading project to convert from assorted collection of MS Word documents to single-sourced XML.\\nResearch Assistant to Dr. Giovanni Parmigiani\\nDUKE UNIVERSITY - Durham NC - 1997 to 1998\\nDesigned hierarchical models / wrote programs in R for Center for Health Policy Research. Identified defects in algorithms and underlying assumptions of Stroke Policy Model leading to creation of web portal that allowed practicing physicians to more\\neasily assess effects of potential treatment strategies utilizing model.\\nEDUCATION\\nMaster of Science in Statistics and Decision Sciences\\nDuke University - Durham NC 1998\\nBaccalaureate in (AB) Mathematics\\nPrinceton University - Princeton NJ 1994\\nMSc in General Strategies for Assessing Convergence of MCMC Algorithms Using Coupled Sample Paths\\nfair assignment of university class rank\\nLINKS http://www.linkedin.com/in/alex-reutter\\n ']\n",
      "\n",
      "['resume_6', 'not_flagged', '\\nPoultney VT - Email me on Indeed: indeed.com/r//d47cbf764c27fba3\\nI am an organized independent worker with strong time management skills. Detail-oriented and able to learn new tasks quickly and effectively.\\nI am a strong and hard working employee who strives to do my best work. I pride myself in respecting my superiors and following directions.\\nHighlights\\nHighly responsible and reliable Upbeat outgoing and positive Works well under pressure Food safety understanding Exceptional interpersonal skills Strongly self motivated Incredibly good at organizational tasks\\nWilling to relocate: Anywhere\\nAuthorized to work in the US for any employer\\nWORK EXPERIENCE\\nMate\\nOsprey Fishing Fleet - New York NY - April 2014 to August 2014\\nPort Jefferson NY\\nNY Osprey Fishing Fleet. Lowest level worker on the boats. Tasks included: basic knowledge of fishing and fishing laws associated with charter boats in this area cutting bait cleaning fish setting up fishing\\nrods (ie. rigging cleaning untangling etc.) general boat upkeep and customer service.\\nVolunteer Project Intern\\nAvalon Park and Preserve - February 2013 to August 2014\\nStony Brook NY\\nAssisted with the Truck Farm local food and gardening educational demonstrations. Jobs included: Organizing events teaching about gardening and edible plants attending local fairs and festivals working on the truck and garden bed doing educational demonstrations.\\nProject Intern\\nAvalon Park and Preserve - October 2013 to December 2013\\nStony Brook NY\\nWorked with research scientists Zosia Baumann and Daniel Madigan in the SBU Marine and\\nAtmospheric Science Department in laboratory. My jobs include subsampling various fish tissues for further tissue analysis among other organizational tasks.\\nStony Brook Dept. of New Literacy and Bhutan Centre for Media and Democracy\\nIntern assistant\\nParo and Thimphu - December 2012 to January 2013\\nBhutan\\nTook photographs of the teacher workshop and happenings around the BCMD office. I also processed data and made that data into usable graphs and video taped interviews of teachers attending the news literacy workshop. I was also required to sort through and count papers. While helping with a four day teacher workshop I learned important elements of news and media literacy.\\nOffice Intern\\n \\nStony Brook University Center for News Literacy - New York NY - August 2012 to August 2012\\nStony Brook New York\\nOrganized and alphabetized office records took calls sorted papers etc.\\nEDUCATION\\nBachelor of Arts in Sustainable Agriculture and Food Production\\nGreen Mountain College - Poultney VT May 2017 to Present\\nWard Melville High School 2014\\nHigh School Diploma\\nGeneral College Preparatory Education -\\nSKILLS\\nSustainable Agriculture (3 years)\\nEast Setauket NY\\n']\n",
      "\n",
      "['resume_7', 'not_flagged', '\\nMedical Laboratory Scientist (Special Chemistry) - Dartmouth College Giesel School of Medicine\\nSouth Royalton VT - Email me on Indeed: indeed.com/r//bbf6b080f47051cd\\nTo secure a position with a reputable organization where I can use my current skills and expertise while developing new skills and utilize all possible available opportunities to pursue a long term career.\\nPERSONAL QUALITIES\\nSelf-motivated result-oriented energetic dedicated\\nCompassionate to patients and their families strong relationship building skills Authorized to work in the US for any employer\\nWORK EXPERIENCE\\nMedical Laboratory Scientist (Special Chemistry)\\nDartmouth College Giesel School of Medicine - Lebanon NH - July 2013 to Present\\n��� Serve as informational resource and lab assistant for UNH students.\\n��� Performs accurate and appropriate testing of specimens received in the laboratory according to established laboratory protocol and procedures.\\n��� Helped with the implementation of new allergy testing on the Thermo Scientific Phadia 250.\\n��� Executed and analyzed tests such as antinuclear antibody through fluorescent microscopy and detection of specific analytes (TTG IgA Quantiferon TB anti-neutrophil cytoplasmic antibody) through enzyme linked immunosorbent assay (ELISA).\\n��� Prepare samples and reagents for immunosuppressant drug monitoring on the Waters Mass Spectrometer. ��� Integrated instrument data quality control and tested principles for accurate result reporting.\\n��� Participate in proficiency testing of unknowns and internal and external continuing education programs to keep current of developments within field.\\n��� Perform clerical and support services as needed such as answering the telephone calling STATS and alerting values to the appropriate department or clinician disposing of contaminated specimens control of inventory etc.\\n��� Aided in the physical move of the laboratory to a new location revalidation of tests and instruments.\\nMedical Laboratory Scientist (Generalist)\\nRutland Regional Medical Center - Rutland VT - June 2012 to July 2013\\nPerforms accurate and appropriate testing of specimens received in the laboratory according to established laboratory protocol and procedures.\\n��� Reports results for stats abnormal assays critical values and other categories of special requests as defined by laboratory policy in clinical areas including chemistry hematology coag blood bank microbiology and reference.\\n��� Accurately analyzed and evaluated QC results obtained before accepting and reporting patient test results. Records results obtained for quality control testing as defined in test procedure.\\n��� Performed maintenance and calibrations according to the schedule for the instrument/equipment.\\n��� Troubleshoots instruments equipment reagents and patient specimens when problems occur.\\n��� Demonstrates respect and regard for the dignity of all patients families visitors and fellow employees to ensure a professional responsible and courteous environment.\\n \\n��� Rotating charge technologist.\\n��� Lead tech on a team that implemented a new lab-wide specimen tracking system.\\nLaboratory Assistant\\nGifford Medical Center - Randolph VT - June 2008 to June 2012\\n��� Perform phlebotomy in outpatient clinic and inpatient areas.\\n��� Trained & competent in all age ranges including pediatrics\\n��� Perform EKG testing in outpatient clinic\\n��� Set up and sanitize laboratories complying with OSHA & other regulatory standards\\n��� Prepare specimens Analyze fluid chemical content collect blood samples & examine elements. ��� Prepare specimens that are required to be sent out to other facilities.\\n��� Well versed in electronic medical records and computerized charting systems\\n��� Clerical skills including answering phones greeting patients and receiving and collecting data entry of specimens\\n��� Highly trained in customer service skills including diffusing agitated patients etc.\\nMedical Laboratory Technologist - Student Intern\\nGlens Falls Hospital - Glens Falls NY - January 2012 to May 2012\\n��� Performed and analyzed tests in clinical areas including chemistry hematology blood bank urinalysis serology histology and bacteriology.\\n��� Ensured test-result validity before recording/reporting results.\\n��� Evaluated quality control within laboratory using standard laboratory test and measurement controls and maintained compliance with CLIA OSHA safety and risk-management guidelines.\\n��� Operated and calibrated an assortment of laboratory/testing equipment and performed various chemical microscopic and bacteriologic tests.\\n��� Analyzers used: Abbott Cell Dyne Sapphire Siemens Dimension Vista Advia Centaur Variant II etc.\\n��� Hands on experience with Cerner Millennium computer system\\nEDUCATION\\nBachelor of Science in Medical Laboratory Science\\nUniversity of Vermont - Burlington VT August 2007 to May 2012\\nCERTIFICATIONS/LICENSES\\nASCP\\nAugust 2018\\nMedical Laboratory scientist\\n']\n",
      "\n",
      "['resume_8', 'flagged', 'Statistician\\nBurlington VT - Email me on Indeed: indeed.com/r//0d7d4fd5131c8662\\nTo secure a position that would allow for growth and development. To work in an environment supportive of reaching goals that would allow me to make a contribution to the organization.\\nWORK EXPERIENCE\\nStatistician\\nIBM - Essex Jct VT - 2010 to July 2013\\nStatistical consulting for semiconductor manufacturing fabricator\\n* Provide statistical consulting services to engineering community.\\n* Statistical process control (SPC) support and guidance.\\n* Measurement systems analysis and support.\\n* Instruction in classes (statistical methods SPC design of experiment) for engineering community\\nSoftware Engineer\\nIBM - Essex Jct VT - 1997 to 2010\\nDevelopment of statistical applications used by engineers to analyze data\\n* Customer support. Help hundreds of data warehouse users access and analyze their data. * Statistical consulting. Provide engineers with the best way to do their analysis.\\n* Education support for proprietary data analysis software.\\n* Technical writing in support of software\\nStaff Engineer /Scientist\\nIBM - Essex Jct VT - 1991 to 1997\\nStatistician for IBM photomask facility. Provide statistical analysis and consulting to the general photomask community.\\n* Engineering community data analysis support. Help the engineers understand the trends in and complexities of process data.\\n* Yield model development. Predict product yield from processing results.\\nEngineer /Scientist\\nIBM - Essex Jct VT - 1990 to 1991\\nCapacity planner - IBM site mainframe usage (VM/MVS). * Usage forecasting out one two and five years\\n* Development of capital plan to meet forecast\\nEDUCATION\\nMS in Statistics\\nUniversity of Vermont - Burlington VT 1991 to 1993\\n BA in Geography\\nMiddlebury College -\\nMiddlebury VT\\n1969 to 1973\\nADDITIONAL INFORMATION SKILLS\\nData Analysis Programming - SAS (http://www.sas.com/) SQL SPSS Technical Writing software customer support and education HTML MS EXCEL\\n']\n",
      "\n",
      "['resume_9', 'not_flagged', \"Research technician\\nBurlington VT - Email me on Indeed: indeed.com/r//37468526e5f0909d\\nI am a young enthusiastic scientist with previous experience in academic research who wants to enter into the biotechnology industry to utilize my scientific skills to investigate molecular disease pathways with the goal of advancing protein therapeutics.\\nWORK EXPERIENCE\\nLaboratory/Research Technician\\nUniversity of Vermont - Burlington VT - August 2011 to Present\\nSupervisor: Maria Roemhildt Ph.D.\\nAssist the PI in performing laboratory experiments focused on an experimental in vivo rat model evaluating primary osteoarthritis disease development. Performed quantification and documentation of histological and genetic investigations. Responsible for manuscript drafting and submission for publication. Submitted and presented research to academic conferences. Maintained position as laboratory safety officer for multiple laboratories within the Orthopaedics Department of the University.\\nSenior Honors Research\\nBiology Department Saint Michael���s College - Winooski VT - September 2010 to May 2011\\nSupervisor: Douglas Green Ph.D.\\nConducted literature review of primary and secondary sources on the evolution of virulence of viruses and the trade-off hypothesis. Developed a working host and pathogen system with E. coli and T4 bacteriophage in order to evaluate the tempo and mode of virulence transmission under various conditions. Prepared poster presentation of results.\\nResearch Grant Recipient\\nBiology Department Saint Michael���s College - Winooski VT - June 2010 to September 2010\\nSupervisor: Douglas Green Ph.D.\\nAdapted standard laboratory techniques in order to develop a standard growth curve currently used to evaluate bacteria population data and MOI.\\nResearch Project\\nSchool for Field Studies - San Carlos CA - September 2009 to December 2009\\nSupervisor: AJ Schneller Ph.D.\\nResearched assessed and documented the outcomes of an experiential environmental learning course designed to engage local seventh grade students in community environmental issues while facilitating their participation in community conservation actions. Exposed to the application process of research submission to academic conferences. Research accepted to be presented at the NAAEE conference.\\nEDUCATION\\nBS in Biology\\nSaint Michael's College - Winooski VT 2007 to 2011\\n \\nSKILLS\\nLab Skills: Brightfield and fluorescent microscopy immunohistochemistry real time RT-PCR analysis\\nslide preparation and staining processing specimens for histological analyses gene expression analyses histological analyses NMR UV/VIS spectroscopy IR spectroscopy gel electrophoresis (SDS-PAGE) paper chromatography column chromatography bacteria culture techniques genotyping/phenotyping Vidana video analysis software ANOVA analysis of variance sterile technique autoclaving dissection staining and culture of microorganisms centrifugation extraction and isolation of DNA/RNA laboratory safety training and implementation. �� Computer experience: Microsoft Word Microsoft Excel Microsoft PowerPoint. �� Languages: Proficient in Spanish 7 years of courses 1 semester in Mexico �� Strong writing analytical and math skills\\nLINKS http://www.ncbi.nlm.nih.gov/pubmed/23123358\\nADDITIONAL INFORMATION\\nHONORS\\n�� Phi Beta Kappa inducted Spring 2011\\n�� Sigma Xi The Scientific Research Society inducted Spring 2011\\n�� Dean���s List Honors Saint Michael���s College [...]\\n�� Saint Michael���s College Honors Scholarship [...]\\n�� Saint Michael���s College Scholarship [...]\\n�� Member of Delta Epsilon Sigma National Honor Society inducted Spring 2010 �� Member of Saint Michael���s Honor Program [...]\\n�� Hartnett Grant Recipient for academic study in science 2010\\nPEER-REVIEWED\\nPUBLICATIONS\\nRoemhildt M. L. Beynnon B. D. Gauthier A. E. Gardner-Morse M. Ertem F. & Badger G. J. (2012). Chronic In Vivo Load Alteration Induces Degenerative Changes in the Rat Tibiofemoral Joint. Osteoarthritis and Cartilage.\\n \"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in data_resume_raw[:10]:\n",
    "    print('{}\\n'.format(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Corrections\n",
    "\n",
    "With the Resume data, I had some trouble with the encoding. It did not seem to be a common encoding, and it looks like basically one character was causing problems, and that character was probably some special bullet character. I don't know if there is a correct encoding that renders it, or that it wasn't properly encoded during data collection.\n",
    "\n",
    "There are common unrecognised characters, \"���\" that appears to be a normal bullet, and \"��\" for another bullet. These replacements by python are not perfect, but we can replace these again with `*` and `**` to have slightly more semantic text. First an example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Customer Service Supervisor/Tier - Isabella Catalog Company\n",
      "South Burlington VT - Email me on Indeed: indeed.com/r//49f8c9aecf490d26\n",
      "WORK EXPERIENCE\n",
      "Customer Service Supervisor/Tier\n",
      "Isabella Catalog Company - Shelburne VT - August 2015 to Present\n",
      "2 Customer Service/Visual Set Up & Display/Website Maintenance\n",
      "��� Supervise customer service team of a popular catalog company\n",
      "��� Manage day to day issues and resolution of customer upset to ensure customer satisfaction\n",
      "��� Troubleshoot order and shipping issues: lost in transit order errors damages\n",
      "��� Manage and resolve escalated customer calls to ensure customer satisfaction\n",
      "��� Assist customers with order placing cross-selling/upselling of catalog merchandise\n",
      "��� Set up and display of sample merchandise in catalog library as well as customer pick-up area of the facility ��� Website clean-up: adding images type up product information proofreading\n",
      "Administrative Assistant /Events Coordinator/Office Services Assistant\n",
      "Eileen Fisher Inc - Irvington NY - February 2014 to July 2015\n",
      "Support to Director of Architecture and Architecture Coordinator in all daily activities including: preparing monthly expense reports scheduling calendar maintenance arranging all aspects of travel/logistics catering interior design research projects\n",
      "��� Manage event set ups through entire process for two Eileen Fisher corporate locations\n",
      "��� Catering overseeing set up walk-thru of space with client review event forms with facilities team ��� Daily management of two professional calendars that require heavy scheduling\n",
      "��� Office services that include: companywide room reservations office supply orders\n",
      "��� Filtered calls to the Chief Creative Officer/Owner of the company\n",
      "Temp Assignment\n",
      "OrthoNet - White Plains NY - December 2013 to February 2014\n",
      "Office Services Assistant/Receptionist\n",
      "��� Managed heavy call volume for orthopedic specialty benefit management company\n",
      "��� Directed heavy daily incoming mail flow\n",
      "��� Processed daily checks and entered data into Excel to generate totals for accounting reports\n",
      "Executive Personal Assistant\n",
      "Westchester NY - January 2012 to December 2013\n",
      "Home Office Assistant/ Personal Assistant\n",
      "��� Provided professional office support to three established Psychologists in the New York area ��� Carefully handled personal and confidential patient information\n",
      "��� Organized uncluttered and simplified office space to create a more user-friendly atmosphere ��� Coordinated and researched all travel related details (flights hotels visas cars etc.)\n",
      "��� Managed personal errands phone calls and emails.\n",
      "��� Responsible for mail processing and bank deposits while Psychologists were traveling\n",
      "Customer Service Representative/ Account Manager\n",
      "CM Almy & Sons Inc - Greenwich CT - January 2007 to January 2012\n",
      " \n",
      "Greenwich CT January 2007 - January 2012\n",
      "Customer Service Representative/ Account Manager\n",
      "��� Provided a high level of customer service to clergy and church members of all denominations\n",
      "��� Answered heavy call volume and assisted customers in a highly efficient manor\n",
      "��� Assisted customers with overall design of garments final decision making of church item purchases ��� Managed and maintained a large account database with daily phone calls to customer accounts\n",
      "��� Responsible for tracking large shipments and also replacement of lost or damaged items\n",
      "435 Dorset Street * South Burlington VT 05403 ��_ 914.564.4381 ��_ Aimeerblair319@gmail.com\n",
      "Administrative Assistant to Chief Financial Officer\n",
      "Coalition to Salute Americas Heroes - Ossining NY - January 2005 to January 2007\n",
      "Ossining NY January 2005 - January 2007\n",
      "Administrative Assistant to Chief Financial Officer\n",
      "��� Interviewed military veterans and their families to be considered for financial aid ��� Reviewed a highly confidential database for candidate\n",
      "��� Mediated discussions between military veterans and collectors\n",
      "��� Arrange final payouts for debt incurred during time of injury\n",
      "��� Finalized paperwork for award payouts\n",
      "��� Coordinated travel and logistics for large sponsored events\n",
      "��� Assisted disabled veterans during events\n",
      "��� Provided basic administrative support\n",
      "Administrative Assistant to Sales Team/ Trade Show Coordinator\n",
      "Leo Electron Microscopy - Thornwood NY - May 2000 to August 2003\n",
      "Thornwood NY May 2000 - August 2003\n",
      "Administrative Assistant to Sales Team/ Trade Show Coordinator\n",
      "��� Communicated general information and provided quotes to high end buyers\n",
      "��� Worked closely with a team of sales associates arranging meetings with potential buyers\n",
      "��� Prepared final proposals and closing sale information on purchased electron microscopes\n",
      "��� Arranged all aspects of travel and logistics for trade shows within the United States and Canada.\n",
      "��� Attended trade shows with sales associates and scientists to insure all electron microscopes arrived safely for set up\n",
      "��� Assisted with demonstrations and close of sales on trade show floor\n",
      "Artist Charles Fazzino 3D Pop Artist\n",
      "Charles Fazzino - New Rochelle NY - 1993 to 1996\n",
      "and 2003-2005\n",
      "Freelance Artist\n",
      "��� Assembled 3 dimensional piece-art on a weekly basis from home office\n",
      "��� Responsible for detailed finishing work and making pieces presentable for purchase in galleries world-wide\n",
      "EDUCATION\n",
      "AAS in Visual Arts\n",
      "Westchester Community College - New York NY School knowledge\n",
      "ADDITIONAL INFORMATION\n",
      "Providing more than 15 years of combined office services with a focus on Administrative Assistance Customer Service Event Coordination Trade Show Coordination and Facilitating\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_resume_raw[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Replacements\n",
    "\n",
    "Make a fixed dataset by replacing these common missing characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_resume_fix = []\n",
    "data_resume_fix.append(data_resume_raw[0])  # header\n",
    "\n",
    "for i, (r_id, r_class, resume_text) in enumerate(data_resume_raw):\n",
    "    if i == 0:\n",
    "        continue  # skip header\n",
    "    resume_text = resume_text.replace(\"���\", \"*\")\n",
    "    resume_text = resume_text.replace(\"��\", \"**\")\n",
    "    data_resume_fix.append([r_id, r_class, resume_text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the same output as above after the fix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Customer Service Supervisor/Tier - Isabella Catalog Company\n",
      "South Burlington VT - Email me on Indeed: indeed.com/r//49f8c9aecf490d26\n",
      "WORK EXPERIENCE\n",
      "Customer Service Supervisor/Tier\n",
      "Isabella Catalog Company - Shelburne VT - August 2015 to Present\n",
      "2 Customer Service/Visual Set Up & Display/Website Maintenance\n",
      "* Supervise customer service team of a popular catalog company\n",
      "* Manage day to day issues and resolution of customer upset to ensure customer satisfaction\n",
      "* Troubleshoot order and shipping issues: lost in transit order errors damages\n",
      "* Manage and resolve escalated customer calls to ensure customer satisfaction\n",
      "* Assist customers with order placing cross-selling/upselling of catalog merchandise\n",
      "* Set up and display of sample merchandise in catalog library as well as customer pick-up area of the facility * Website clean-up: adding images type up product information proofreading\n",
      "Administrative Assistant /Events Coordinator/Office Services Assistant\n",
      "Eileen Fisher Inc - Irvington NY - February 2014 to July 2015\n",
      "Support to Director of Architecture and Architecture Coordinator in all daily activities including: preparing monthly expense reports scheduling calendar maintenance arranging all aspects of travel/logistics catering interior design research projects\n",
      "* Manage event set ups through entire process for two Eileen Fisher corporate locations\n",
      "* Catering overseeing set up walk-thru of space with client review event forms with facilities team * Daily management of two professional calendars that require heavy scheduling\n",
      "* Office services that include: companywide room reservations office supply orders\n",
      "* Filtered calls to the Chief Creative Officer/Owner of the company\n",
      "Temp Assignment\n",
      "OrthoNet - White Plains NY - December 2013 to February 2014\n",
      "Office Services Assistant/Receptionist\n",
      "* Managed heavy call volume for orthopedic specialty benefit management company\n",
      "* Directed heavy daily incoming mail flow\n",
      "* Processed daily checks and entered data into Excel to generate totals for accounting reports\n",
      "Executive Personal Assistant\n",
      "Westchester NY - January 2012 to December 2013\n",
      "Home Office Assistant/ Personal Assistant\n",
      "* Provided professional office support to three established Psychologists in the New York area * Carefully handled personal and confidential patient information\n",
      "* Organized uncluttered and simplified office space to create a more user-friendly atmosphere * Coordinated and researched all travel related details (flights hotels visas cars etc.)\n",
      "* Managed personal errands phone calls and emails.\n",
      "* Responsible for mail processing and bank deposits while Psychologists were traveling\n",
      "Customer Service Representative/ Account Manager\n",
      "CM Almy & Sons Inc - Greenwich CT - January 2007 to January 2012\n",
      " \n",
      "Greenwich CT January 2007 - January 2012\n",
      "Customer Service Representative/ Account Manager\n",
      "* Provided a high level of customer service to clergy and church members of all denominations\n",
      "* Answered heavy call volume and assisted customers in a highly efficient manor\n",
      "* Assisted customers with overall design of garments final decision making of church item purchases * Managed and maintained a large account database with daily phone calls to customer accounts\n",
      "* Responsible for tracking large shipments and also replacement of lost or damaged items\n",
      "435 Dorset Street * South Burlington VT 05403 **_ 914.564.4381 **_ Aimeerblair319@gmail.com\n",
      "Administrative Assistant to Chief Financial Officer\n",
      "Coalition to Salute Americas Heroes - Ossining NY - January 2005 to January 2007\n",
      "Ossining NY January 2005 - January 2007\n",
      "Administrative Assistant to Chief Financial Officer\n",
      "* Interviewed military veterans and their families to be considered for financial aid * Reviewed a highly confidential database for candidate\n",
      "* Mediated discussions between military veterans and collectors\n",
      "* Arrange final payouts for debt incurred during time of injury\n",
      "* Finalized paperwork for award payouts\n",
      "* Coordinated travel and logistics for large sponsored events\n",
      "* Assisted disabled veterans during events\n",
      "* Provided basic administrative support\n",
      "Administrative Assistant to Sales Team/ Trade Show Coordinator\n",
      "Leo Electron Microscopy - Thornwood NY - May 2000 to August 2003\n",
      "Thornwood NY May 2000 - August 2003\n",
      "Administrative Assistant to Sales Team/ Trade Show Coordinator\n",
      "* Communicated general information and provided quotes to high end buyers\n",
      "* Worked closely with a team of sales associates arranging meetings with potential buyers\n",
      "* Prepared final proposals and closing sale information on purchased electron microscopes\n",
      "* Arranged all aspects of travel and logistics for trade shows within the United States and Canada.\n",
      "* Attended trade shows with sales associates and scientists to insure all electron microscopes arrived safely for set up\n",
      "* Assisted with demonstrations and close of sales on trade show floor\n",
      "Artist Charles Fazzino 3D Pop Artist\n",
      "Charles Fazzino - New Rochelle NY - 1993 to 1996\n",
      "and 2003-2005\n",
      "Freelance Artist\n",
      "* Assembled 3 dimensional piece-art on a weekly basis from home office\n",
      "* Responsible for detailed finishing work and making pieces presentable for purchase in galleries world-wide\n",
      "EDUCATION\n",
      "AAS in Visual Arts\n",
      "Westchester Community College - New York NY School knowledge\n",
      "ADDITIONAL INFORMATION\n",
      "Providing more than 15 years of combined office services with a focus on Administrative Assistance Customer Service Event Coordination Trade Show Coordination and Facilitating\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_resume_fix[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Adapters\n",
    "\n",
    "Both Datasets appear to be three strings, with the format:\n",
    "\n",
    "ID, class label, example text\n",
    "\n",
    "In the Chatbot case, there are 5 trailing empty columns, and the example text is a user chat response. In the Resume case, the example text is a long text body representing a resume. There doesn't appear to be a common format of the resumes. The class labels are the same for both cases, `flagged` or `not_flagged`. Just the meaning of these labels changes for each dataset. The ID is also a text string, not a number.\n",
    "\n",
    "### DataManager\n",
    "\n",
    "DataManager is a class I wrote before for the very common task of dividing data into train/valid/test sets in a ratio specified by the `split` tuple. This is not the cleanest implementation, but by sub-classing `DataManager` an adapter for a custom dataset can be built. In the NLP case, we don't want to one-hot encode at this stage, but retain the text labels. The text will go through vocabulary ID tokenisation later on.\n",
    "\n",
    "To customise `DataManager` a few modifications need to be made. In `__init__`, the dataset_path should be set where the files will be output, and whether to `discard_header` or not should be set. Then two methods need to be overridden. `_process_row_raw(self, row)` processes the raw CSV data. So it needs to be modified to match the format of the CSV data we have. It will write a file from this for train/valid/test sets, so the output format should be specified, including any pre-processing, like making an ID from the labels. `_process_row_split(self, row)` reads in the files written above, and prepares the data into a format that will actually be used for training/validating/testing. This division allows us to store more information about the data than we finally use in our model.\n",
    "\n",
    "### Chatbot\n",
    "\n",
    "Firstly reading in and splitting the Chatbot data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_tools as dt\n",
    "from data_tools import DataManager\n",
    "\n",
    "class DeepNLPChatbotData(DataManager):\n",
    "    def __init__(self, filepath, split, one_hot_encode=True, output_numpy=True):\n",
    "        super().__init__(filepath, split, one_hot_encode, output_numpy)\n",
    "        self.filepath = filepath\n",
    "        self.split = split       # train/valid/test fractions, should sum to 1\n",
    "        self.dataset_path = 'data/deepnlp/chatbot/'\n",
    "        self.discard_header = True\n",
    "\n",
    "    def _process_row_raw(self, row):\n",
    "        \"\"\"\n",
    "        Import lines from raw data file.\n",
    "        Imports line of \"Text ID, label, text\"\n",
    "        Returns list of [Text ID, text, label, label_index]\n",
    "        \"\"\"\n",
    "        read_line = []\n",
    "        text_id   = row[0]\n",
    "        label     = row[1]\n",
    "        text      = row[2]\n",
    "        label_idx = self._get_idx(label)\n",
    "        read_line.extend([text_id, text, label, label_idx])\n",
    "        return read_line\n",
    "\n",
    "    def _process_row_split(self, row):\n",
    "        \"\"\"\n",
    "        Import lines from train/valid/test split files.\n",
    "        Imports line of \"Text ID, text, label, label_index\"\n",
    "        Returns list of [text, label]\n",
    "        \"\"\"\n",
    "        read_line = []\n",
    "\n",
    "        text      = row[1]\n",
    "        label     = row[2]\n",
    "        label_idx = int(float(row[-1]))\n",
    "        fetch_idx = self._get_idx(label)  # rebuilds num_classes\n",
    "        read_line.extend([text, label])\n",
    "        return read_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Train/Valid/Test data from data/deepnlp/sheet_1.csv\n",
      "Split train has ( 44, 20, ) examples of each class\n",
      "Split test has ( 1, 1, ) examples of each class\n",
      "Split valid has ( 10, 4, ) examples of each class\n",
      "Dataset prepared\n"
     ]
    }
   ],
   "source": [
    "filepath = 'data/deepnlp/sheet_1.csv'\n",
    "data_manager = DeepNLPChatbotData(filepath, (0.8, 0.19, 0.01), one_hot_encode=False, output_numpy=False)\n",
    "data_manager.init_dataset()\n",
    "chatbot_train_x, chatbot_train_y = data_manager.prepare_train()\n",
    "chatbot_valid_x, chatbot_valid_y = data_manager.prepare_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "Since this dataset is very small, I've decided to take most of the `test` split into the `validation` split. So we get a train/valid/test split of 80%/19%/1%. With a dataset this small, I think all that can be done is model development, and we're not really at a stage of formally 'testing' the model.\n",
    "\n",
    "With these divisions, we still only get 64 training examples and 14 validation examples. There are about double the amount of `not_flagged` examples compared to the number of `flagged` examples. So we can expect the model may learn some bias of not flagging examples. 14 examples is not much to validate against, so the validation accuracy reported may not be so reliable. Rather it just gives us some indication of how the model will generalise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Sometimes I'll calm my friends down after bad stuff happens.\",\n",
       " 'Ex girlfriend had depression and anxiety. I used to hold her and listen as she told me what was going on',\n",
       " 'GF and I help her through a lot of shit because I myself have been through a lot of shit.',\n",
       " \"I used to tutor homeless men at a shelter to help them obtain their GED's. They were all age 50+ and some of them were even reading at a first grade level.\")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_train_x[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('not_flagged', 'flagged', 'not_flagged', 'not_flagged')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_train_y[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume\n",
    "\n",
    "Now build the same for the resume dataset. This will require more edits due to the encoding problems. Here we do the odd character replacement as we process each row in the original CSV file. Also, since in this case our original file has some encoding issues, the `_data_import()` method was also overridden to `replace` the unrecognised characters, instead of stopping with an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepNLPResumeData(DataManager):\n",
    "    def __init__(self, filepath, split, one_hot_encode=True, output_numpy=True):\n",
    "        super().__init__(filepath, split, one_hot_encode, output_numpy)\n",
    "        self.filepath = filepath\n",
    "        self.split = split       # train/valid/test fractions, should sum to 1\n",
    "        self.dataset_path = 'data/deepnlp/resume/'\n",
    "        self.discard_header = True\n",
    "\n",
    "    def _process_row_raw(self, row):\n",
    "        \"\"\"\n",
    "        Import lines from raw data file.\n",
    "        Imports line of \"Text ID, label, text\"\n",
    "        Returns list of [Text ID, text, label, label_index]\n",
    "        \"\"\"\n",
    "        read_line = []\n",
    "        text_id   = row[0]\n",
    "        label     = row[1]\n",
    "        text      = row[2]\n",
    "        label_idx = self._get_idx(label)\n",
    "        \n",
    "        # make unknown character replacements (tends to be bullets)\n",
    "        text = text.replace(\"���\", \"*\")\n",
    "        text = text.replace(\"��\", \"**\")\n",
    "        \n",
    "        read_line.extend([text_id, text, label, label_idx])\n",
    "        return read_line\n",
    "\n",
    "    def _process_row_split(self, row):\n",
    "        \"\"\"\n",
    "        Import lines from train/valid/test split files.\n",
    "        Imports line of \"Text ID, text, label, label_index\"\n",
    "        Returns list of [text, label]\n",
    "        \"\"\"\n",
    "        read_line = []\n",
    "\n",
    "        text      = row[1]\n",
    "        label     = row[2]\n",
    "        label_idx = int(float(row[-1]))\n",
    "        fetch_idx = self._get_idx(label)  # rebuilds num_classes\n",
    "        read_line.extend([text, label])\n",
    "        return read_line\n",
    "    \n",
    "    # Override _data_import() method to account for encoding issues\n",
    "    def _data_import(self, filepath):\n",
    "        \"\"\"\n",
    "        Import data from data file.\n",
    "        Override this _process_row_raw() method in child class for dataset structure.\n",
    "        Return a list of [property_1, property_2, ..., target_index]\n",
    "        \"\"\"\n",
    "        data_raw = []\n",
    "        with open(filepath, errors='replace') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for i, row in enumerate(reader):\n",
    "                if not row:\n",
    "                    continue\n",
    "                if self.discard_header and i == 0:\n",
    "                    continue\n",
    "                read_line = self._process_row_raw(row)\n",
    "                data_raw.append(read_line)\n",
    "        return data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Train/Valid/Test data from data/deepnlp/sheet_2.csv\n",
      "Split train has ( 73, 26, ) examples of each class\n",
      "Split test has ( 2, 1, ) examples of each class\n",
      "Split valid has ( 17, 6, ) examples of each class\n",
      "Dataset prepared\n"
     ]
    }
   ],
   "source": [
    "filepath = 'data/deepnlp/sheet_2.csv'\n",
    "data_manager = DeepNLPResumeData(filepath, (0.8, 0.19, 0.01), one_hot_encode=False, output_numpy=False)\n",
    "data_manager.init_dataset()\n",
    "resume_train_x, resume_train_y = data_manager.prepare_train()\n",
    "resume_valid_x, resume_valid_y = data_manager.prepare_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "As with the Chatbot case this dataset is also small, so we've also taken a train/valid/test split of 80%/19%/1% to increase the reliability of validation while maintaining some number of examples for training.\n",
    "We end up with 99 training examples and 23 validation examples. This time there are about 3 times the amount of `not_flagged` examples compared to the number of `flagged` examples. So again a bias towards `not_flagged` is expected. We also do not have the luxury of reducing the number of examples to balance between the classes. In any case, this bias towards not flagging is perhaps the real form of this data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Elizabeth Conway\\nSeeking a Part time Admin/receptionist postition\\nEast Hardwick VT - Email me on Indeed: indeed.com/r/Elizabeth-Conway/f0620353200ab872\\nWORK EXPERIENCE\\nAssistant Calf Manager\\nFairvue Farm - January 2014 to Present\\nDaily calf care on a 2000 cow dairy farm including but not limited to feeding vaccines record keeping to maintain and raise healthy replacement calves assisting dairy manager in creating needed reports.\\nOwner/Operator of a fiber mill\\nFibers - 2007 to 2013\\nResponsibilities included bookkeeping record keeping\\nshipping/receiving customer service maintenance sales daily operations and advertising\\nBookkeeper/Administration\\nE.F. Jones LLC - 2007 to 2013\\nResponsibilities included AP/AR reconciliation and daily administration duties\\nSenior Associate Scientist\\nPfizer Inc - 2004 to 2007\\nResponsibilities included cell culture conducting In Vivo studies\\ntissue collection western blots data analysis and reporting and presenting data to the team leaders.\\nEDUCATION\\nMaster of Animal Science\\nUniversity of Connecticut 2001 to 2003\\nBachelor of Animal Science\\nUniversity of Connecticut 1999 to 2001\\nAnimal Science\\nUniversity of Connecticut 1997 to 1999\\nSKILLS\\nWord Excel Outlook Quickbooks data entry customer service bookeeping filing animal care and sewing\\n ',\n",
       " '\\nIndependent Consultant - Self-employed\\nBurlington VT - Email me on Indeed: indeed.com/r/Alex-Reutter/2c4a904a891a6fef\\nWORK EXPERIENCE\\nIndependent Consultant\\nSelf-employed - Burlington VT - October 2016 to Present Projects in progress.\\nSenior Data Scientist\\nIBM - 2015 to 2016\\nDeveloped product strategies for Data Science Experience (datascience.ibm.com) for machine learning algorithms and end-to- end usage for data scientists. Patient Zero for ensuring product design matches typical data scientist workflows.\\n* Enabled connection with tens-of-thousands of customers onsite and in social media impressions by crafting story of data\\nscientist in auto industry and how use of SPSS and Spark evolved from analysis of spreadsheet data on defect rates of auto\\nparts to integration of PySpark into analytic work streams. Story was featured at 2015 Spark Signature Moment in IBM\\nSpeaker Presenter\\nIBM - 2013 to 2015\\nand in customer-facing videos and presentations.\\n* Increased sales by evangelizing IBM analytics solutions to hundreds of customers onsite as Speaker Presenter at IBM\\nInsight 2013 - 2015.\\n* Improved development of new product features including Data Science Experience (DSX) Watson Analytics SPSS\\nAdvisory Software Engineer (Data Scientist)\\nIBM - 2009 to 2015\\nLed team as Agile product owner for Analytic Server working with offering management and internal and external clients to coordinate and prioritize the development of new software features.\\n* Facilitated quarterly (rather than yearly) release cycles and single integrated system for project tracking by transitioning\\n100+ developers from Waterfalls to Agile development processes as project focal.\\n* Aided transformation of integrated supply chain (ISC) code base from R to more-easily maintainable SPSS by mentoring\\ncolleague on SPSS products from model prototyping to advanced forecasting techniques. Success story of collaboration between software group and ISC was presented to IBM analytics and inspired further internal partnerships.\\n* Delivered Analytic Server 1.0 - 2.1 product by keeping development focused on important stories designing and implementing scripts to track sprint progress and analyze product backlog culling dead work items and celebrating\\nsuccesses.\\n \\nR. ALEXANDER (ALEX) REUTTER alex.reutter@gmail.com\\nMaster Statistical Writer\\nSPSS INC - 2006 to 2009\\nAuthored and contributed to designs integration standards and user-facing documentation of common statistical components across SPSS products.\\n* Gave customers access to item response models otherwise unavailable in SPSS Statistics product by creating custom\\ncommands and dialogs using open source Python and R programmability.\\n* Ensured quality and consistency of thousands of pages of documentation training and sales and marketing presentations by writing cross-product standards for developing examples and sample data that prove product functionality.\\n* Reduced time to create command syntax documentation 80% and ensured designs / documentation were in sync by writing standards for single-sourcing command syntax designs and user-facing documentation.\\nSenior Statistical Writer\\nSPSS INC - 1998 to 2006\\nGave customers access to 1K+ pages of algorithms documentation in easily accessible online Help and PDF format by leading project to convert from assorted collection of MS Word documents to single-sourced XML.\\nResearch Assistant to Dr. Giovanni Parmigiani\\nDUKE UNIVERSITY - Durham NC - 1997 to 1998\\nDesigned hierarchical models / wrote programs in R for Center for Health Policy Research. Identified defects in algorithms and underlying assumptions of Stroke Policy Model leading to creation of web portal that allowed practicing physicians to more\\neasily assess effects of potential treatment strategies utilizing model.\\nEDUCATION\\nMaster of Science in Statistics and Decision Sciences\\nDuke University - Durham NC 1998\\nBaccalaureate in (AB) Mathematics\\nPrinceton University - Princeton NJ 1994\\nMSc in General Strategies for Assessing Convergence of MCMC Algorithms Using Coupled Sample Paths\\nfair assignment of university class rank\\nLINKS http://www.linkedin.com/in/alex-reutter\\n ')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_train_x[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('not_flagged', 'flagged')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_train_y[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Vocabulary\n",
    "\n",
    "The `Vocabulary` class from `data_tools` is used to build the vocabulary for these two datasets. This class is described in more detail in the Spooky Author Identification notebook (`notebook_spooky`). Basically it will build up a vocabulary from corpus for the sentences, and also for the labels. The vocabulary is a list of words ordered by frequency, and truncated to a `vocabulary_size` (20000 here). The word to ID and ID to word conversions are built with the `get_sentence_vocabulary()` and `get_label_vocabulary()` methods. Then the sentences and labels of the dataset can be converted to their tokenised ID form with `data_to_token_ids()` and `labels_to_token_ids()`.\n",
    "The sentences and labels are then combined into one list, `chat_train_set` for train and `chat_valid_set` for validation.\n",
    "\n",
    "### Vocabulary and ID Tokenisation of the Chatbot Dataset\n",
    "\n",
    "Firstly processing the Chatbot dataset. The output files will go to `data/deepnlp/chatbot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary\n",
      "Writing data/deepnlp/chatbot/vocab_sentences.txt ...\n",
      "Writing data/deepnlp/chatbot/vocab_labels.txt ...\n",
      "Writing data/deepnlp/chatbot/train/sentences.txt ...\n",
      "Writing data/deepnlp/chatbot/train/ids_sentences.txt ...\n",
      "Writing data/deepnlp/chatbot/train/labels.txt ...\n",
      "Writing data/deepnlp/chatbot/train/ids_labels.txt ...\n",
      "Writing data/deepnlp/chatbot/valid/sentences.txt ...\n",
      "Writing data/deepnlp/chatbot/valid/ids_sentences.txt ...\n",
      "Writing data/deepnlp/chatbot/valid/labels.txt ...\n",
      "Writing data/deepnlp/chatbot/valid/ids_labels.txt ...\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "vocab = dt.Vocabulary('data/deepnlp/chatbot', 20000)\n",
    "vocab.build_vocabulary(chatbot_train_x, chatbot_train_y)\n",
    "\n",
    "chat_sents_vocab, chat_rev_sents_vocab = vocab.get_sentence_vocabulary()\n",
    "chat_label_vocab, chat_rev_label_vocab = vocab.get_label_vocabulary()\n",
    "\n",
    "chat_train_x_tok = vocab.data_to_token_ids(chatbot_train_x, 'train')\n",
    "chat_train_y_tok = vocab.labels_to_token_ids(chatbot_train_y, 'train')\n",
    "\n",
    "chat_valid_x_tok = vocab.data_to_token_ids(chatbot_valid_x, 'valid')\n",
    "chat_valid_y_tok = vocab.labels_to_token_ids(chatbot_valid_y, 'valid')\n",
    "\n",
    "chat_train_set = list(zip(chat_train_x_tok, chat_train_y_tok))\n",
    "chat_valid_set = list(zip(chat_valid_x_tok, chat_valid_y_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some examples from the tokenised ID set converted back into their word forms, firstly from the training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating token IDs back into words:\n",
      "\n",
      "Sometimes I ' ll calm my friends down after bad stuff happens .\n",
      "not_flagged\n",
      "\n",
      "\n",
      "Ex girlfriend had depression and anxiety . I used to hold her and listen as she told me what was going on\n",
      "flagged\n",
      "\n",
      "\n",
      "GF and I help her through a lot of shit because I myself have been through a lot of shit .\n",
      "not_flagged\n",
      "\n",
      "\n",
      "I used to tutor homeless men at a shelter to help them obtain their GED ' s . They were all age 00+ and some of them were even reading at a first grade level .\n",
      "not_flagged\n",
      "\n",
      "\n",
      "Don ' t have a specific example but just letting people know you ' re there if they want to talk .\n",
      "not_flagged\n",
      "\n",
      "\n",
      "\n",
      "Actual form of training data:\n",
      " ([190, 4, 7, 82, 221, 14, 20, 83, 101, 253, 125, 166, 2], 1)\n"
     ]
    }
   ],
   "source": [
    "print('Translating token IDs back into words:\\n')\n",
    "vocab.translate_examples(chat_train_set[:5])\n",
    "print('\\nActual form of training data:\\n', chat_train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick look at the validation examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_UNK camp , _UNK kids have the same _UNK . I _UNK them i how it is when you cant listen or are _UNK .\n",
      "not_flagged\n",
      "\n",
      "\n",
      "when my best friends _UNK _UNK away from _UNK ' _UNK when he was in grade 0\n",
      "flagged\n",
      "\n",
      "\n",
      "I once _UNK as a resource for someone who was struggling in school , and I helped them with their _UNK .\n",
      "not_flagged\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab.translate_examples(chat_valid_set[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "Seems that the vocabulary is a bit small and a lot of words are being missed in the validation set. Perhaps calculating the vocabulary over all the data would help, but then the validation set has less checking capability for generalisation. Generally, the dataset is a bit too small. One other possibility for improvement is to use some pre-trained word vectors from word2vec or Glove, since we're dealing with English here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary and ID Tokenisation of the Resume Dataset\n",
    "\n",
    "Carry out the same process for the Resume Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sentence corpus in data/deepnlp/resume/sentences_raw.txt\n",
      "Writing data/deepnlp/resume/sentences_raw.txt ...\n",
      "Building vocabulary\n",
      "Writing data/deepnlp/resume/vocab_sentences.txt ...\n",
      "Writing data/deepnlp/resume/vocab_labels.txt ...\n",
      "Writing data/deepnlp/resume/train/sentences.txt ...\n",
      "Writing data/deepnlp/resume/train/ids_sentences.txt ...\n",
      "Writing data/deepnlp/resume/train/labels.txt ...\n",
      "Writing data/deepnlp/resume/train/ids_labels.txt ...\n",
      "Writing data/deepnlp/resume/valid/sentences.txt ...\n",
      "Writing data/deepnlp/resume/valid/ids_sentences.txt ...\n",
      "Writing data/deepnlp/resume/valid/labels.txt ...\n",
      "Writing data/deepnlp/resume/valid/ids_labels.txt ...\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "vocab = dt.Vocabulary('data/deepnlp/resume', 20000)\n",
    "vocab.build_vocabulary(resume_train_x, resume_train_y)\n",
    "\n",
    "resume_sents_vocab, resume_rev_sents_vocab = vocab.get_sentence_vocabulary()\n",
    "resume_label_vocab, resume_rev_label_vocab = vocab.get_label_vocabulary()\n",
    "\n",
    "resume_train_x_tok = vocab.data_to_token_ids(resume_train_x, 'train')\n",
    "resume_train_y_tok = vocab.labels_to_token_ids(resume_train_y, 'train')\n",
    "\n",
    "resume_valid_x_tok = vocab.data_to_token_ids(resume_valid_x, 'valid')\n",
    "resume_valid_y_tok = vocab.labels_to_token_ids(resume_valid_y, 'valid')\n",
    "\n",
    "resume_train_set = list(zip(resume_train_x_tok, resume_train_y_tok))\n",
    "resume_valid_set = list(zip(resume_valid_x_tok, resume_valid_y_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the Resume training set, converted back into words.\n",
    "For the resumes, the example text is quite long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating token IDs back into words:\n",
      "\n",
      "Elizabeth Conway Seeking a Part time Admin/receptionist postition East Hardwick VT - Email me on Indeed : indeed . com/r/Elizabeth-Conway/f0000000000ab000 WORK EXPERIENCE Assistant Calf Manager Fairvue Farm - January 0000 to Present Daily calf care on a 0000 cow dairy farm including but not limited to feeding vaccines record keeping to maintain and raise healthy replacement calves assisting dairy manager in creating needed reports . Owner/Operator of a fiber mill Fibers - 0000 to 0000 Responsibilities included bookkeeping record keeping shipping/receiving customer service maintenance sales daily operations and advertising Bookkeeper/Administration E . F . Jones LLC - 0000 to 0000 Responsibilities included AP/AR reconciliation and daily administration duties Senior Associate Scientist Pfizer Inc - 0000 to 0000 Responsibilities included cell culture conducting In Vivo studies tissue collection western blots data analysis and reporting and presenting data to the team leaders . EDUCATION Master of Animal Science University of Connecticut 0000 to 0000 Bachelor of Animal Science University of Connecticut 0000 to 0000 Animal Science University of Connecticut 0000 to 0000 SKILLS Word Excel Outlook Quickbooks data entry customer service bookeeping filing animal care and sewing\n",
      "not_flagged\n",
      "\n",
      "\n",
      "Independent Consultant - Self-employed Burlington VT - Email me on Indeed : indeed . com/r/Alex-Reutter/0c0a000a000a0fef WORK EXPERIENCE Independent Consultant Self-employed - Burlington VT - October 0000 to Present Projects in progress . Senior Data Scientist IBM - 0000 to 0000 Developed product strategies for Data Science Experience ( datascience . ibm . com ) for machine learning algorithms and end-to- end usage for data scientists . Patient Zero for ensuring product design matches typical data scientist workflows . * Enabled connection with tens-of-thousands of customers onsite and in social media impressions by crafting story of data scientist in auto industry and how use of SPSS and Spark evolved from analysis of spreadsheet data on defect rates of auto parts to integration of PySpark into analytic work streams . Story was featured at 0000 Spark Signature Moment in IBM Speaker Presenter IBM - 0000 to 0000 and in customer-facing videos and presentations . * Increased sales by evangelizing IBM analytics solutions to hundreds of customers onsite as Speaker Presenter at IBM Insight 0000 - 0000 . * Improved development of new product features including Data Science Experience ( DSX ) Watson Analytics SPSS Advisory Software Engineer ( Data Scientist ) IBM - 0000 to 0000 Led team as Agile product owner for Analytic Server working with offering management and internal and external clients to coordinate and prioritize the development of new software features . * Facilitated quarterly ( rather than yearly ) release cycles and single integrated system for project tracking by transitioning 000+ developers from Waterfalls to Agile development processes as project focal . * Aided transformation of integrated supply chain ( ISC ) code base from R to more-easily maintainable SPSS by mentoring colleague on SPSS products from model prototyping to advanced forecasting techniques . Success story of collaboration between software group and ISC was presented to IBM analytics and inspired further internal partnerships . * Delivered Analytic Server 0 . 0 - 0 . 0 product by keeping development focused on important stories designing and implementing scripts to track sprint progress and analyze product backlog culling dead work items and celebrating successes . R . ALEXANDER ( ALEX ) REUTTER alex . reutter@gmail . com Master Statistical Writer SPSS INC - 0000 to 0000 Authored and contributed to designs integration standards and user-facing documentation of common statistical components across SPSS products . * Gave customers access to item response models otherwise unavailable in SPSS Statistics product by creating custom commands and dialogs using open source Python and R programmability . * Ensured quality and consistency of thousands of pages of documentation training and sales and marketing presentations by writing cross-product standards for developing examples and sample data that prove product functionality . * Reduced time to create command syntax documentation 00% and ensured designs / documentation were in sync by writing standards for single-sourcing command syntax designs and user-facing documentation . Senior Statistical Writer SPSS INC - 0000 to 0000 Gave customers access to 0K+ pages of algorithms documentation in easily accessible online Help and PDF format by leading project to convert from assorted collection of MS Word documents to single-sourced XML . Research Assistant to Dr . Giovanni Parmigiani DUKE UNIVERSITY - Durham NC - 0000 to 0000 Designed hierarchical models / wrote programs in R for Center for Health Policy Research . Identified defects in algorithms and underlying assumptions of Stroke Policy Model leading to creation of web portal that allowed practicing physicians to more easily assess effects of potential treatment strategies utilizing model . EDUCATION Master of Science in Statistics and Decision Sciences Duke University - Durham NC 0000 Baccalaureate in ( AB ) Mathematics Princeton University - Princeton NJ 0000 MSc in General Strategies for Assessing Convergence of MCMC Algorithms Using Coupled Sample Paths fair assignment of university class rank LINKS http : //www . linkedin . com/in/alex-reutter\n",
      "flagged\n",
      "\n",
      "\n",
      "Nicholas Snelling Leader - NES Rentals Colchester VT - Email me on Indeed : indeed . com/r/Nicholas-Snelling/ffb0dbd000000000 WORK EXPERIENCE Leader NES Rentals - Williston VT - 0000 to Present in $00B equipment rental industry with 00 locations across Central and Eastern United States . Intern Working with Environmental Compliance Group at different locations nationwide to ensure compliance is being met at all sites . * Completed thorough safety audit at Plattsburgh NY location . * Compiled data from all branches to compile Compliance Safety and Accountability ( CSA ) scores of third party haulers . * Edited all safety webinars and Material Safety Data Sheet ( MSDS ) for new hire webinar . * Compiled data from all branches on waste receptacles . * Created multiple surveys in Survey Monkey sent to all branch managers . * Completed Aerial Work Platform Training ( AWPTA ) and CPR/AED training . * Helped roll out new mobile applications for all drivers allowing them to log hours digitally . * Reviewed Spill Prevention Control and Countermeasure ( SPCC ) plan proposal and suggested recommendations . Burlington Country Club - Burlington VT - June 0000 to August 0000 Private family friendly country club offering highest standards of golf and hospitality in New England Greens Staff - June 0000 to August 0000 Helped maintain country club grounds . Cleaned bunkers mowed lawn and helped maintain native grass areas . Chipotle Restaurant - South Burlington VT - June 0000 to August 0000 Opened and closed entire kitchen area . Prepared all food served in restaurant . Helped open new store in area . Worked with management team to ensure excellent customer dining experience . Prep Cook City Market - Burlington VT - June 0000 to August 0000 Burlington VT Summer 0000 Community-owned grocery store in offering local organic and conventional products as well as hot bar and made to order counter . Prep Cook Created and executed a new menu for customers everyday along with maintaining a clean working kitchen . Took inventory and helped create food orders . Maintained presentable buffet style counter area . Kitchen Manager New York Pizza Oven - 0000 to 0000 New York Pizza Oven - Colchester VT - 0000 to 0000 New York Style pizzeria that also served Italian foods such as pasta along with chicken wings and subs . Fry Cook New York Pizza Oven - 0000 to 0000 Managed and maintained working kitchen by providing guidance to all staff members as well as taking full responsibility for all earnings that were acquired during shift . * Helped design and introduce new menu . * Made nightly cash deposits along with accounting for card/cash earnings . * Selected as part of group of employees used to open new restaurant . * Managed 0-0 employees during shift . EDUCATION Marine Science Maine Maritime Academy ADDITIONAL INFORMATION QUALIFICATIONS Analytical organized and efficient Marine Scientist with laboratory experience in : * Monitoring Chemical Systems * Preparing & Analyzing Samples * Operating Spectrophotometer * Centrifuge Usage * Chemical Analysis * Properly Disposing Hazardous Materials * Solution Mixtures * Running SPSS Analysis Tests * Maintaining Laboratory Inventory * Compiling Test Results * Writing Technical Reports * Interpreting Test Results Accomplish tasks in timely and accurate ways as instructed by team leaders . Able to analyze tasks to produce efficient results . Quick learner who enjoys incorporating new learning into practical applications . Resolve problems using critical thinking skills . Work well independently and in team environments .\n",
      "not_flagged\n",
      "\n",
      "\n",
      "Data Entry Data Entry - JP Morgan Chase S Burlington VT - Email me on Indeed : indeed . com/r/0e0000d00b0ddb00 To be employed as a bench scientist in either the private or public sector to apply my skills and training to practice good science and solve problems of economic scientific and/or social importance . WORK EXPERIENCE Data Entry JP Morgan Chase - South Burlington VT - June 0000 to Present Au Pair Roma Lazio - March 0000 to May 0000 Teaching Assistant HORT Clemson University School of Agricultural - Clemson SC - May 0000 to December 0000 Clemson SC May 0000 - December 0000 Teaching Assistant HORT 000/000 : Just Fruits Fall 0000 -Involved in lecture preparation and the gathering of materials for demonstrations and sampling -Managed and updated the Blackboard Learning System page for the course -Reviewed and revised quiz exam and syllabus material for the course Research Assistant Clemson University School of Agricultural - May 0000 to December 0000 Organized and prepared samples for analysis through freeze drying using a lyophilizer and grinding samples using liquid nitrogen -Used a BioPhotometer to calculate DNA concentration in a sample and vacuum centrifuge to reconcentrate samples -Performed enzyme digestions PCR ( including basic knowledge of Real Time PCR ) and gel electrophoresis -Performed DNA extraction procedures Field Work -Collected compiled and analyzed field samples and data -Performed laboratory analysis of fruit for qualities such as firmness size coloration chlorophyll content pH and soluble solids -Collected data using the Fruit Texture Analyzer refractometer titration sampler DA meter and colorimeter EDUCATION Bachelor of Science in Genetics Clemson University - Clemson SC December 0000\n",
      "not_flagged\n",
      "\n",
      "\n",
      "Leonid Alexander Norsworthy PhD IEMBA Former online Professor/Knowledge Manager Saint Johnsbury VT - Email me on Indeed : indeed . com/r/000000cb00e00e00 International educator/economic development professional with over 00 years of experience . Offers extensive experience in knowledge management and learning corporate strategy and learning/informatics program design and implementation . Willing to relocate : Anywhere Authorized to work in the US for any employer WORK EXPERIENCE Adjunct Professor/Collegiate Professor University of Maryland - 0000 to 0000 Graduate School of Management and Technology Adelphi Maryland Taught Seminar 000 Contemporary/Competitive Strategy Analysis the final seminars in 00 semester hour Master of Business Administration program . Mentored fellow adjunct professor to assist in improvements in student evaluation scores . Assisted with transition of syllabus from 00 week to 00 week format for two final 0 graduate credit hour seminars . Taught 000 Environment and Organizations . Adjunct Professor University of Maryland - 0000 to 0000 Taught 0000 Germany in World Affairs 0000 Russia and Eastern Europe in World Affairs 0000 Western Europe in World Affairs 0000 Contemporary American Foreign Policy International Political Economy Adjunct Online Professor/Senior Online Instructor Norwich University - Northfield VT - 0000 to 0000 College of Graduate and Continuing Studies Teach Seminar One : Theory in the International System Seminar Three : Law in the International System Seminar Four Commerce and the International System . Grade comprehensive examinations . Participate in June residencies culmination of six 0 semester hour seminars for Master of Diplomacy program . Graded comprehensive examinations . Supervised Masters theses . Participated in annual June residencies for Master of Arts in Diplomacy graduating students in Northfield Vermont . 0000 Visiting adjunct professor IUKB Institut Universitaire Kurt Bosch New York College Athens Greece/SUNY Empire State MBA in International Business and MBA in Marketing programs . Invited to teach 0 graduate credit EU based seminar with English as the language of instruction . Adjunct Professor Department of Business Administration and Department of Social Studies - Lyndonville VT - 0000 to 0000 Taught 0000 International Business required course for Bachelor of Business Administration program . Introduced case methods and systematic development of critical thinking and other competencies Consultant/Writer-Editor/Knowledge Manager WORLD BANK GROUP - Washington DC - October 0000 to March 0000 Informatics and Knowledge Manager conducted public policy and economic research on transitional economies of Europe Central Asia Latin America and Caribbean and lower income countries in Africa . Authored and edited strategy documents reports best practice notes and issue briefings . Led teams responsible for project and tasks in global development learning project research and analysis web applications and content development news services records and document management publications and corporate communications . Managed and implemented knowledge initiatives developed publications and communications strategies to reach stakeholders specialists and development partners . Senior Consultant--Writer/Editor WORLD BANK GROUP - 0000 to 0000 Latin America Caribbean Region Operations Evaluation Department Environmentally and Socially Sustainable Development * Through interviews with specialists and document reviews and synthesis exercises launch publication En Breve to disseminate information on noteworthy Bank lending and non-lending activities in the Latin America Caribbean region . * Developed Results at a Glance publication for external audiences on the impact of Bank work in the areas of clean energy biodiversity and natural resources management water and agricultural productivity . * Review sector operations and lessons from experience to draft sector guidelines for the use of Social Analysis in natural resources management and rural development projects and activities . * Interview sector specialists to identify priorities and strategic thrusts of operations including relevant analytical frameworks . * Coordinate drafting of overviews and sample terms of reference with sector specialists and social scientists for inclusion in Social Analysis Sourcebook . Adjunct Professor Global Development Program - Washington DC - 0000 to 0000 Knowledge Manager WORLD BANK GROUP - 0000 to 0000 * Managed projects to establish distance learning centers in 00 countries . Oversaw market studies to determine demand for distance learning centers in Bosnia Bulgaria Lithuania Poland Russia and Ukraine . Content areas were in public sector management privatization and economic reform . * In consultation with clients developed governance and resource mobilization strategies for these centers to achieve sustainable operations in knowledge services for economic development . Facilitated dialogue with high-ranking government officials and corporate leaders and other stakeholders on ICT issues relevant to knowledge economy and other Bank initiatives . * Pioneered innovative approach to leverage existing national technological and human capital bases to reduce costs and integrate knowledge services within a given country in the region . Contributed to Bank science and technology strategy . * Developed technical platforms for disseminating knowledge products and provided guidance and support to the task teams and thematic groups . Promoted and developed knowledge initiatives and partnerships with external client institutions . Implemented integrated web site knowledge system and promoted the use of these tools among staff . Provided guidance on operational and budgetary issues developing custom applications for project tracking . * After training and certification served as coordinator for introduction of new financial products for lending operations and introduction of SAP and report development to support operations . * Supervised knowledge management and distance learning teams of information analysts operations analysts and project consultants . Associate Adjunct Professor School of International Service & School of Public Affairs - Consultant Economist/Social Scientist-other WORLD BANK GROUP - 0000 to 0000 Washington DC - 0000 to 0000 Sub-Saharan Africa Region Environmentally and Socially Sustainable Development & Human Development Departments * Designed and implemented new Rural Development and Environment knowledge management system including the publishing of web pages and the synthesizing of key documents on lessons learned in the field . Reviewed and analyzed findings of NRM and Rural Development projects . * Developed new knowledge management systems for Health Population and Nutrition Affinity Groups in the Africa Region . Updated and designed new Poverty Monitoring knowledge management system . * Wrote proposals to bilateral aid and multilateral donors in support of population and poverty projects for the African Population Advisory Committee located in the World Bank raising US$ 0 million . * Developed and implemented document dissemination strategy for case studies training materials newsletters and videos for public health and poverty reduction activities in West and East Africa . * Developed Staff Appraisal Report for Senegal Sector Investment Project integrating consultant studies and materials from preparatory field work . Wrote political component of extended poverty study of the Republic of Guinea . Developed presentations on Bamako Initiative public health financing health care and education reforms for the region as a whole . Authored press briefings for World Bank President ' s visit to Africa . Director Foreign Policy Studies Daiwa Institute of Research/Daiwa Securities America - Washington DC - April 0000 to June 0000 As a senior policy analyst and manager in the consulting subsidiary of the world ' s second largest securities firm responsible for business development and monitoring the areas of U . S . foreign policy financial services regulation international trade defense and science and technology policy . * Wrote daily weekly and monthly reports and books on U . S . policies affecting investor interests in the United States and around the world . Organized fact-finding missions for Daiwa personnel and clients to meet with industry leaders association executives and key federal government officials . * Monitored and responded to World Bank Inter-American Development Bank and US AID contract opportunities for Daiwa and U . S . joint venture partners . Developed proposals for joint ventures and federal procurement contracts in the areas of information technology privatization technical assistance and financial services modernization . * Developed successful proposal for ADB Indonesia workforce study . Participated in successful bid for EBRD Far East Venture Capital Fund in NIS . Promoted EBRD IBRD-funded training program in privatization and financial services for NIS managers and government officials * Organized and conducted Washington components of postal service information systems procurement practices . Conducted and disseminated findings of study on U . S . systems integration services . Investigated and documented joint venture opportunities between US and Japanese companies in Asia and Latin America . Conceptualized and implemented fact finding tours for Japanese financial executives to study reforms in the U . S . banking and securities industries . Instructor School of International Service & School of Public Affairs - Washington DC - 0000 to 0000 Director Development Serivices Catholi University of America - Washington DC - November 0000 to April 0000 As a member of the capital campaign management team managed fund accounting prospect research and systems in support of a $000 million centennial campaign and $00 million in annual development revenues . Lecturer Catholic University of America--University College - Washington DC - 0000 to 0000 Taught two sections of Europe and 0000 ManagerMarketing Research American Enterprise Institute - Washington DC - June 0000 to October 0000 First as a consultant and then as a member of the development staff responsible for corporate relations representing two-thirds of a $00 million annual budget conducting extensive marketing research and managing a data conversion from a IBM System/00 to a Novell network using Clipper and dBase software . Assistant Director Development - Washington DC - 0000 to 0000 Responsible for managing prospect research for a $000 million campaign . Coordinated direct mail solicitations fund raising scripts proposals to foundations corporations and individuals . . EDUCATION International Master of Business Administration in Business Adminisration McDonough School of Business - Washington DC 0000 to 0000 Doctor of Philosophy in International Relations American University - Washington DC 0000 to 0000 Master of Arts in International Affars American University - Washington DC January 0000 to December 0000 Bachelor of Arts in International Studies American University - Washington DC September 0000 to December 0000 SKILLS Foreign Lanagues : French German Russian Spanish and Portuguese ( 00+ years ) Microsoft Office ( 00+ years ) Lotus Notes/Domino ( 00+ years ) Foreign Language : Russian ( 00+ years ) Foreign Language German ( 00+ years ) Foreign Language Spanish ( 00+ years ) Graduate online graduate school instruction ( 00+ years ) AWARDS Beta Gamma Sigma Georgetown University Washington D . C . May 0000 Top 00% of 00 student graduating cohort in IEMBA program Phi Kappa Phi May 0000 Top 0% of all graduating graduate students at American University Deans List Amerivan University December 0000 Graduate with a 0 . 0 grade point average for all undergraduate coursework completed at American University PUBLICATIONS Russian Views of the Transition April 0000 Co-wrote and co-edited compendium of Russian authors views of the transition to a market economy . Lessons from Experience : Agriculture Natural Resources and the Environemnt March 0000 co-wrote adn co-edited compendium of best practices in the rural development natural resources management and environment management sector forr the Environmentally and socially Sustainable Development professional network in the Europe Central Asia region of the World Bank Group ( International Bank for Reconstruction and Development ) The Clinton Revolution : an Insider ' s Look at the New Administration January 0000 Co-wrote and co-edited review of teh foreign and domestic policies of the incoming Clinton Administration the first book on Bill Clinton ' s administration published in Japan and appearing in the United States of America . Dawn of a New Era October 0000 Co-wrote and co-edited a policy review/rounup of the bush Administration in preparation for the 000 presidential election . Non-Profit Computer Sourcebook . 0000 For the Taft Group researched wrote and edited a directory of computer software and hardware systems available for non-profits FRI Prospect Research Resource Directory October 0000 Researched wrote and edited directory of resources for prospect research for non-profitd engaged in annual and capital fund raising campaigns . Political Economy of Science and Technolgy in the German Democratic Republic May 0000 Doctoral dissertation on science and technology in the German Democratic Republic . Findings based on literature reviews and multivariate regressions of foreign trade and domestic production in the micro-elecronics sector in East Germany . Originally predicted reunification going into defense in February 0000 but was counseled to change this prediction to a more neutral set of findings without predictions of political economic policy predictions East Germany Trade and Economic Policies Master ' s thesis written in support of Master of Arts degree from the Scholl of Interntional Service awarded in December 0000 ADDITIONAL INFORMATION Full list of publications presentations and references available upon request\n",
      "not_flagged\n",
      "\n",
      "\n",
      "\n",
      "Actual form of training data:\n",
      " ([7466, 11181, 2583, 13, 2647, 146, 9963, 10961, 979, 9845, 18, 8, 63, 52, 17, 65, 16, 64, 3, 11031, 66, 62, 46, 9972, 119, 8642, 1531, 8, 70, 6, 4, 61, 3031, 7338, 327, 17, 13, 6, 5724, 2550, 991, 34, 1145, 610, 3069, 4, 1499, 6728, 813, 934, 4, 405, 2, 1861, 2805, 3175, 10626, 1195, 2550, 637, 10, 582, 291, 101, 3, 4484, 5, 13, 4592, 8575, 8841, 8, 6, 4, 6, 124, 108, 10801, 813, 934, 11942, 125, 201, 199, 231, 415, 209, 2, 2836, 7536, 506, 3, 1187, 3, 3760, 907, 8, 6, 4, 6, 124, 108, 7836, 11245, 2, 415, 1080, 413, 167, 341, 91, 10696, 137, 8, 6, 4, 6, 124, 108, 219, 1076, 957, 242, 5347, 225, 768, 255, 4897, 2721, 20, 31, 2, 432, 2, 9579, 20, 4, 9, 68, 761, 3, 71, 268, 5, 1124, 43, 22, 5, 1214, 6, 4, 6, 207, 5, 1124, 43, 22, 5, 1214, 6, 4, 6, 1124, 43, 22, 5, 1214, 6, 4, 6, 138, 383, 197, 1846, 7676, 20, 904, 125, 201, 10725, 1777, 503, 327, 2, 6381], 1)\n"
     ]
    }
   ],
   "source": [
    "print('Translating token IDs back into words:\\n')\n",
    "vocab.translate_examples(resume_train_set[:5])\n",
    "print('\\nActual form of training data:\\n', resume_train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job _UNK Staff _UNK Field Geologist - _UNK Environmental / _UNK Technical Services - Email me on Indeed : indeed . _UNK WORK EXPERIENCE Staff _UNK Field Geologist _UNK Environmental / _UNK Technical Services - Montpelier VT - 0000 to Present Conducted site investigations using the _UNK _UNK and _UNK _UNK Interface Probe . Trained new _UNK employees on field services . Seasonal employee _UNK Environmental - Montpelier VT - 0000 to 0000 Worked for the _UNK and _UNK departments as a lead field sampler . Also worked with many ArcGIS projects for the _UNK _UNK and Water Resources departments . Undergraduate Teaching Assistant University of Vermont - Burlington VT - 0000 to 0000 Helped teach Geology 0 labs . EDUCATION B . S . in Geology University of Vermont 0000 CERTIFICATIONS/LICENSES CPR & First Aid ADDITIONAL INFORMATION Technical Skills _UNK core logging . _UNK core logging . _UNK _UNK logging . _UNK sampling . _UNK and rock sampling . _UNK Mapping . _UNK . -Data management and _UNK . _UNK maintenance / engine repair . _UNK electronics repair . Jacob Vincent I joined _UNK Environmental in the spring of 0000 working with the Agricultural Chemistry Water Resources and Applied Information Management Groups . _UNK graduating from the University of Vermont in 0000 with a B . S . in Geology and a minor in Geospatial Technologies I became a member of the Investigation and Remediation Team at _UNK Environmental . I am responsible for performing _UNK advanced profiling core discrete _UNK network and membrane interface probe investigations as well as _UNK data management .\n",
      "not_flagged\n",
      "\n",
      "\n",
      "_UNK _UNK Senior Research Biologist Bradford VT - Email me on Indeed : indeed . _UNK WORK EXPERIENCE Senior Scientist _UNK Inc - Lebanon NH - June 0000 to April 0000 _UNK the supervision of Dr . Robert _UNK Principal _UNK Fellow _UNK : o Contributed to the development of _UNK _UNK a unique _UNK antibody and therapeutic protein expression platform . o Supported efforts focused on _UNK vaccine development utilizing glycoengineered yeast for the evaluation optimization and production of novel vaccine candidates . o Involved in the development of a bioinformatics _UNK to optimize _UNK ' s _UNK production system that including : 0 ) development and _UNK protocols to support RNA profiling and next generation sequencing ; 0 ) follow up of identified _UNK using _UNK genetics . o Responsible for hands-on expression of monoclonal antibodies using glycoengineered yeast host including optimization of critical quality attributes to support lead ID development programs . o _UNK molecular biology work to improve the host strains for optimal therapeutic protein production that includes primer design PCR cloning sequence analysis DNA isolation as well as standard yeast microbiology ( e . g . transformation screening and isolation of yeast strains _UNK blot assays ) and immunohistochemistry techniques . Senior Research Associate _UNK Inc - Lebanon NH - November 0000 to June 0000 Associate to Dr . Robert _UNK o Contributed to one of the six original company specific _UNK o Participated in external collaboration efforts to produce and optimize drug discovery candidate proteins in _UNK _UNK strains . o Supported independent research efforts including development of key tools such as _UNK and _UNK expression _UNK optimization of yeast strain _UNK expression of _UNK and exploration of alternative yeast expression systems . Research Assistant I _UNK Lab Dartmouth Medical School - May 0000 to November 0000 Participated in a large scale project to _UNK 00000 genes in _UNK _UNK adapting PCR yeast _UNK and _UNK protocols to create high-throughput methods that were efficient and consistent on a liquid handling robot . Research Assistant I _UNK _UNK - August 0000 to May 0000 PhD UT Southwestern Medical Center * Carried out various molecular biology and biochemical techniques to central experiments in the lab such as DNA and RNA purification PCR Northern _UNK Southern _UNK Western _UNK RT-PCR preparation of protein basic tissue culture immunohistochemistry and _UNK * Researched developed and facilitated new techniques * Supervised summer undergraduate students in lab procedures and safety protocols * Provided assistance to graduate students fellows and _UNK working in the lab * Maintained records for supplies _UNK protocols freezer contents and lab reagents * Supervised mouse colony _UNK Dallas Aquarium - Dallas TX - March 0000 to August 0000 _UNK part time Maritime Aquarium - _UNK CT - June 0000 to February 0000 EDUCATION B . S . in Biology Southern Connecticut State University May 0000 B . A . in History University of Connecticut December 0000 SKILLS Strong Molecular biology techniques in animal and yeast models ADDITIONAL INFORMATION Research associate with over 00 years progressive academic and industrial experience in molecular research and discovery . Advanced experience in creating new research protocols supporting ongoing studies and _UNK data for presentation . Highly responsible and dedicated team player with excellent leadership and _UNK skills .\n",
      "not_flagged\n",
      "\n",
      "\n",
      "Engineer / Scientist - IBM Microelectronics Division Westford VT - Email me on Indeed : indeed . _UNK WORK EXPERIENCE Engineer / Scientist IBM Microelectronics Division - June 0000 to Present Responsible for Process and Equipment engineering for multiple lines including : o Multiple _UNK processes o _UNK and lead free _UNK _UNK o Thermal _UNK * Wrote specifications and _UNK capital for equipment purchases and upgrades * Project management for equipment installation and equipment upgrades * Developed methods for acquiring and tracking critical data metrics * _UNK production efficiency _UNK through data-driven decision making * Implemented and maintained Lean Manufacturing initiatives o Root cause analysis and _UNK Problem solving o _UNK Improvement activities o Standard Work and Job _UNK _UNK * Utilized statistical process controls on critical process indicators * Worked with cross functional teams to implement and communicate changes * Improved process flow and reduced cycle time through waste _UNK * Identified opportunities for and _UNK implementation of technical improvements to current manufacturing processes and procedures * Oversight and implementation of multiple complex projects simultaneously * Responsible for quality inspection strategy including inspection methods sample plans and defect criteria ( June 0000 - July 0000 ) * Oversight of materials transport processes ( June 0000 - July 0000 ) Engineering Technician IBM Microelectronics Division - June 0000 to June 0000 June 0000 ) * _UNK and Process support for : o _UNK mask cleaning and inspection processes o Multiple _UNK processes o _UNK and lead free _UNK _UNK o Thermal _UNK * _UNK manufacturing processes * Trained production operators * Qualified new equipment and processes Production Operator IBM Microelectronics Division - May 0000 to May 0000 _UNK Operator IBM Microelectronics Division - September 0000 to May 0000 Supervisor _UNK Contract Manufacturing - October 0000 to September 0000 Supervised 00 employees working in quality control * Handled personnel issues including scheduling _UNK planning hiring firing performance evaluations and resource actions _UNK Inspector _UNK Contract Manufacturing - March 0000 to October 0000 _UNK Operator IBM - February 0000 to October 0000 EDUCATION Bachelors in Business Technology and Management Vermont Technical College Associates in General Engineering Technologies Vermont Technical College ADDITIONAL INFORMATION Skills * Microsoft Office Suite Lotus _UNK Windows AIX familiar with _UNK SQL other database software and Microsoft Project * PLC _UNK _UNK _UNK Omega _UNK Laser Particle _UNK Microscopes Automated Inspection Equipment Belt _UNK Blue M _UNK Thermal _UNK _UNK * Lean Manufacturing initiatives _UNK Problem solving 0S continuous improvement value stream analysis standard work visual _UNK systems * Excellent presentation skills public speaking power point\n",
      "not_flagged\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab.translate_examples(resume_valid_set[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LSTM Model\n",
    "\n",
    "Below the class that builds a custom LSTM model in TensorFlow has been included. More details of this model have been explained in the `notebook_spooky` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel(object):\n",
    "    def __init__(self, h_size, num_layers, vocab_size, n_classes, batch_size, rnn_type='lstm'):\n",
    "\n",
    "        # Input Placeholders\n",
    "        self.x = x = tf.placeholder(tf.int32, [batch_size, None], name=\"inputs\") # [batch_size, num_steps]\n",
    "        self.seqlen = seqlen = tf.placeholder(tf.int32, [batch_size], name=\"sequence_lengths\")\n",
    "        self.y = y = tf.placeholder(tf.int32, [batch_size], name=\"classes_gt\")\n",
    "        self.keep_prob = keep_prob = tf.placeholder(\"float\")\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        def cell_gen():\n",
    "            return tf.contrib.rnn.BasicLSTMCell(h_size, state_is_tuple=True)\n",
    "        if rnn_type == 'gru':\n",
    "            def cell_gen():\n",
    "                return tf.contrib.rnn.GRUCell(h_size)\n",
    "        \n",
    "        if num_layers > 1:\n",
    "            cells = []\n",
    "            for _ in range(num_layers):\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(cell_gen(), output_keep_prob=keep_prob)\n",
    "                cells.append(cell)\n",
    "        \n",
    "            cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        else:\n",
    "            cell = cell_gen()\n",
    "        \n",
    "        self.cell = cell\n",
    "        \n",
    "        # TODO: Prepare init state\n",
    "#         # Initialise one hidden state\n",
    "#         init_state = tf.get_variable('init_state', [1, h_size],\n",
    "#                                  initializer=tf.constant_initializer(0.0))\n",
    "#         # Tile to match batch_size\n",
    "#         init_state = tf.tile(init_state, [batch_size, 1])\n",
    "#         print(init_state)\n",
    "        \n",
    "        # Embedding layer\n",
    "        embeddings = tf.get_variable('embedding_matrix', [vocab_size, h_size])\n",
    "        rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "        \n",
    "#         rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "#                                                      initial_state=init_state)\n",
    "        rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen, dtype=tf.float32)\n",
    "\n",
    "        #idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "        #last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)        \n",
    "        last_rnn_output = tf.gather_nd(rnn_outputs, tf.stack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "        # Softmax layer\n",
    "        with tf.variable_scope('softmax'):\n",
    "            W = tf.get_variable('W', [h_size, n_classes])\n",
    "            b = tf.get_variable('b', [n_classes], initializer=tf.constant_initializer(0.0))\n",
    "        logits = tf.matmul(last_rnn_output, W) + b\n",
    "        preds = tf.nn.softmax(logits)\n",
    "        correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "        self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "        self.train_step = tf.train.AdamOptimizer(1e-4).minimize(self.loss, global_step=self.global_step)\n",
    "        \n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "        \n",
    "        self._prepare_logs()\n",
    "        \n",
    "    def _prepare_logs(self):\n",
    "        tf.summary.scalar('Loss', self.loss)\n",
    "        tf.summary.scalar('Accuracy', self.accuracy)\n",
    "        \n",
    "        self.logs = tf.summary.merge_all()\n",
    "\n",
    "def create_model(session, logdir, **parameters):\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        print('\\nCreating model with parameters:')\n",
    "        for k,v in parameters.items():\n",
    "            print('{:16s}: {}'.format(k, v))\n",
    "        model_train = RNNModel(parameters['h_size'], parameters['rnn_layers'], FLAGS_in_vocab_size,\n",
    "                               FLAGS_n_classes, parameters['batch_size'])\n",
    "        \n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    #print(ckpt.model_checkpoint_path)\n",
    "    if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path + '.index'):\n",
    "        print(\"Loading model from parameters in {}.\".format(ckpt.model_checkpoint_path))\n",
    "        model_train.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Creating model with fresh parameters.\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    return model_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Code\n",
    "\n",
    "Below is the training and validation code as used before in the `notebook_spooky` case, where it has been explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "FLAGS_in_vocab_size = 20000\n",
    "FLAGS_n_classes = 2\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def valid_eval(sess, model, valid_set, batches):\n",
    "    total_steps = 0\n",
    "    val_accuracy = 0\n",
    "    val_loss = 0\n",
    "    for epoch in batches.gen_padded_batch_epochs(valid_set, 1):\n",
    "        for step, (batch_x, batch_y, lengths) in enumerate(epoch):\n",
    "            total_steps += 1\n",
    "            feed = {model.x: batch_x, model.y: batch_y, model.seqlen: lengths, model.keep_prob: 1.0}\n",
    "            fetch = [model.accuracy, model.loss]\n",
    "            \n",
    "            val_accuracy_, val_loss_ = sess.run(fetch, feed_dict=feed)\n",
    "            val_accuracy += val_accuracy_\n",
    "            val_loss += val_loss_\n",
    "    avg_val_accuracy = val_accuracy / total_steps\n",
    "    avg_val_loss = val_loss / total_steps\n",
    "    \n",
    "    return avg_val_accuracy, avg_val_loss\n",
    "\n",
    "\n",
    "\n",
    "def train_net(train_set, valid_set, n_epochs, run_name, **params):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        log_dir = os.path.join(FLAGS_log_dir, run_name)\n",
    "        log_txt_dir = os.path.join(FLAGS_log_dir, 'txtlogs/')\n",
    "        if not os.path.exists(log_txt_dir):\n",
    "            os.makedirs(log_txt_dir)\n",
    "        \n",
    "        model = create_model(sess, log_dir, **params)\n",
    "        \n",
    "        # Dropout keep neuron output probability\n",
    "        keep_prob = params['dropout_keep']\n",
    "        \n",
    "        batches = dt.Batches(params['batch_size'])\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "        valid_writer = tf.summary.FileWriter(log_dir + '/valid', sess.graph)\n",
    "        \n",
    "        quantities = ['Gstep', 'Accuracy', 'Loss', 'Time']\n",
    "        train_logs = dt.Logger(*quantities)\n",
    "        valid_logs = dt.Logger(*quantities)\n",
    "\n",
    "        start_time = timer()\n",
    "\n",
    "        for i, epoch in enumerate(batches.gen_padded_batch_epochs(train_set, n_epochs)):\n",
    "            print('\\nEpoch', i+1)\n",
    "            accuracy = 0\n",
    "            loss = 0\n",
    "            for step, (batch_x, batch_y, lengths) in enumerate(epoch):\n",
    "\n",
    "                feed = {model.x: batch_x, model.y: batch_y, model.seqlen: lengths, model.keep_prob: keep_prob}\n",
    "                fetch = [model.accuracy, model.loss, model.logs, model.train_step]\n",
    "\n",
    "                accuracy_, loss_, logs, _ = sess.run(fetch, feed_dict=feed)\n",
    "                accuracy += accuracy_\n",
    "                loss += loss_\n",
    "\n",
    "                gstep = model.global_step.eval()\n",
    "                elapsed = timer() - start_time\n",
    "\n",
    "                train_writer.add_summary(logs, gstep)\n",
    "                train_logs.log(Gstep=gstep, Accuracy=accuracy_, Loss=loss_, Time=elapsed)\n",
    "\n",
    "                if step % n_steps_avg == 0 and step > 0:\n",
    "                    avg_accuracy = accuracy/n_steps_avg\n",
    "                    avg_loss = loss/n_steps_avg\n",
    "                    print('Step {:6d}, accuracy: {:7.3f}, loss: {:7.3f}, {:6.1f}s elapsed ({} steps avg.)'.format(\n",
    "                        gstep, avg_accuracy, avg_loss, elapsed, n_steps_avg))\n",
    "                    accuracy = 0\n",
    "                    loss = 0                          \n",
    "\n",
    "            valid_accuracy, valid_loss = valid_eval(sess, model, valid_set, batches)\n",
    "            print('Global Step {}, valid accuracy: {:7.3}'.format(gstep, valid_accuracy))\n",
    "            \n",
    "            elapsed = timer() - start_time\n",
    "            valid_logs.log(Gstep=gstep, Accuracy=valid_accuracy, Loss=valid_loss, Time=elapsed)\n",
    "\n",
    "            summary = tf.Summary()\n",
    "            summary.value.add(tag=\"model/Accuracy\", simple_value=valid_accuracy)\n",
    "            summary.value.add(tag=\"model/Loss\", simple_value=valid_loss)\n",
    "            valid_writer.add_summary(summary, gstep)\n",
    "            valid_writer.flush()\n",
    "\n",
    "            tf.logging.info('Step {} validation accuracy: {:7.3}'.format(gstep, valid_accuracy))\n",
    "\n",
    "            checkpoint_path = os.path.join(log_dir, 'crm_lstm.ckpt')\n",
    "            model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "\n",
    "        print('Done Training')\n",
    "\n",
    "        train_logs.write_csv(os.path.join(log_txt_dir, run_name + '_train.csv'))\n",
    "        valid_logs.write_csv(os.path.join(log_txt_dir, run_name + '_valid.csv'))\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning\n",
    "\n",
    "Code for tuning multiple hyper-parameters. Since our dataset is so small, we will just try one very simple model. With \n",
    "\n",
    "- 256 hidden units\n",
    "- 1 LSTM layer\n",
    "- batches of 2 examples\n",
    "- 70% neuron output, with 30% dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter sets\n",
    "import itertools\n",
    "\n",
    "class ParameterTuner(object):\n",
    "    def __init__(self):\n",
    "        self.h_sizes = None\n",
    "        self.rnn_layers = None\n",
    "        self.batch_sizes = None\n",
    "        self.dropout_keep = None\n",
    "    \n",
    "    def n_sets(self):\n",
    "        return len(self.h_sizes)*len(self.rnn_layers)*len(self.batch_sizes)*len(self.dropout_keep)\n",
    "    \n",
    "    def sets(self):\n",
    "        parameters = [self.h_sizes, self.rnn_layers, self.batch_sizes, self.dropout_keep]\n",
    "        for h, layers, batches, dropouts in itertools.product(*parameters):\n",
    "            par_set = {}\n",
    "            par_set['h_size'] = h\n",
    "            par_set['rnn_layers'] = layers\n",
    "            par_set['batch_size'] = batches\n",
    "            par_set['dropout_keep'] = dropouts\n",
    "            par_string = 'h{}_l{}_b{}_d{}'.format(h, layers, batches, dropouts)\n",
    "            yield par_set, par_string\n",
    "\n",
    "# h_sizes = [128, 256, 512, 1024]\n",
    "# rnn_layers = [1, 2, 3, 4]\n",
    "# batch_sizes = [16, 32, 64, 128]\n",
    "\n",
    "# h_sizes = [128, 256]\n",
    "# rnn_layers = [1, 2]\n",
    "# batch_sizes = [16, 32]\n",
    "\n",
    "h_sizes = [256]\n",
    "rnn_layers = [1]\n",
    "batch_sizes = [2]\n",
    "dropout_keep = [0.7]\n",
    "\n",
    "\n",
    "tuner = ParameterTuner()\n",
    "tuner.h_sizes = h_sizes\n",
    "tuner.rnn_layers = rnn_layers\n",
    "tuner.batch_sizes = batch_sizes\n",
    "tuner.dropout_keep = dropout_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Training\n",
    "\n",
    "Firstly, lets try to train this model on the Chatbot dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Run 1/1, 0.0001356780412606895s elapsed\n",
      "\n",
      "Creating model with parameters:\n",
      "h_size          : 256\n",
      "batch_size      : 2\n",
      "rnn_layers      : 1\n",
      "dropout_keep    : 0.7\n",
      "Loading model from parameters in logs_chatbot/h256_l1_b2_d0.7/crm_lstm.ckpt-320.\n",
      "INFO:tensorflow:Restoring parameters from logs_chatbot/h256_l1_b2_d0.7/crm_lstm.ckpt-320\n",
      "\n",
      "Epoch 1\n",
      "Step    331, accuracy:   1.050, loss:   0.203,    1.8s elapsed (10 steps avg.)\n",
      "Step    341, accuracy:   1.000, loss:   0.115,    3.3s elapsed (10 steps avg.)\n",
      "Step    351, accuracy:   1.000, loss:   0.071,    4.4s elapsed (10 steps avg.)\n",
      "Global Step 352, valid accuracy:     0.5\n",
      "INFO:tensorflow:Step 352 validation accuracy:     0.5\n",
      "\n",
      "Epoch 2\n",
      "Step    363, accuracy:   1.100, loss:   0.099,    7.2s elapsed (10 steps avg.)\n",
      "Step    373, accuracy:   1.000, loss:   0.069,    8.3s elapsed (10 steps avg.)\n",
      "Step    383, accuracy:   0.950, loss:   0.146,    9.7s elapsed (10 steps avg.)\n",
      "Global Step 384, valid accuracy:   0.429\n",
      "INFO:tensorflow:Step 384 validation accuracy:   0.429\n",
      "\n",
      "Epoch 3\n",
      "Step    395, accuracy:   1.100, loss:   0.083,   13.1s elapsed (10 steps avg.)\n",
      "Step    405, accuracy:   1.000, loss:   0.128,   14.1s elapsed (10 steps avg.)\n",
      "Step    415, accuracy:   1.000, loss:   0.079,   15.7s elapsed (10 steps avg.)\n",
      "Global Step 416, valid accuracy:   0.571\n",
      "INFO:tensorflow:Step 416 validation accuracy:   0.571\n",
      "\n",
      "Epoch 4\n",
      "Step    427, accuracy:   1.100, loss:   0.089,   18.6s elapsed (10 steps avg.)\n",
      "Step    437, accuracy:   1.000, loss:   0.074,   20.0s elapsed (10 steps avg.)\n",
      "Step    447, accuracy:   1.000, loss:   0.083,   21.2s elapsed (10 steps avg.)\n",
      "Global Step 448, valid accuracy:   0.429\n",
      "INFO:tensorflow:Step 448 validation accuracy:   0.429\n",
      "\n",
      "Epoch 5\n",
      "Step    459, accuracy:   1.100, loss:   0.048,   24.5s elapsed (10 steps avg.)\n",
      "Step    469, accuracy:   1.000, loss:   0.108,   25.5s elapsed (10 steps avg.)\n",
      "Step    479, accuracy:   1.000, loss:   0.055,   26.7s elapsed (10 steps avg.)\n",
      "Global Step 480, valid accuracy:   0.429\n",
      "INFO:tensorflow:Step 480 validation accuracy:   0.429\n",
      "\n",
      "Epoch 6\n",
      "Step    491, accuracy:   1.100, loss:   0.076,   29.2s elapsed (10 steps avg.)\n",
      "Step    501, accuracy:   1.000, loss:   0.047,   30.5s elapsed (10 steps avg.)\n",
      "Step    511, accuracy:   1.000, loss:   0.061,   32.9s elapsed (10 steps avg.)\n",
      "Global Step 512, valid accuracy:     0.5\n",
      "INFO:tensorflow:Step 512 validation accuracy:     0.5\n",
      "\n",
      "Epoch 7\n",
      "Step    523, accuracy:   1.100, loss:   0.056,   35.4s elapsed (10 steps avg.)\n",
      "Step    533, accuracy:   1.000, loss:   0.060,   36.8s elapsed (10 steps avg.)\n",
      "Step    543, accuracy:   1.000, loss:   0.048,   38.5s elapsed (10 steps avg.)\n",
      "Global Step 544, valid accuracy:     0.5\n",
      "INFO:tensorflow:Step 544 validation accuracy:     0.5\n",
      "\n",
      "Epoch 8\n",
      "Step    555, accuracy:   1.100, loss:   0.026,   41.1s elapsed (10 steps avg.)\n",
      "Step    565, accuracy:   1.000, loss:   0.064,   42.5s elapsed (10 steps avg.)\n",
      "Step    575, accuracy:   1.000, loss:   0.059,   44.2s elapsed (10 steps avg.)\n",
      "Global Step 576, valid accuracy:     0.5\n",
      "INFO:tensorflow:Step 576 validation accuracy:     0.5\n",
      "\n",
      "Epoch 9\n",
      "Step    587, accuracy:   1.100, loss:   0.058,   47.1s elapsed (10 steps avg.)\n",
      "Step    597, accuracy:   1.000, loss:   0.046,   49.1s elapsed (10 steps avg.)\n",
      "Step    607, accuracy:   1.000, loss:   0.035,   50.5s elapsed (10 steps avg.)\n",
      "Global Step 608, valid accuracy:   0.571\n",
      "INFO:tensorflow:Step 608 validation accuracy:   0.571\n",
      "\n",
      "Epoch 10\n",
      "Step    619, accuracy:   1.100, loss:   0.057,   53.5s elapsed (10 steps avg.)\n",
      "Step    629, accuracy:   1.000, loss:   0.047,   55.4s elapsed (10 steps avg.)\n",
      "Step    639, accuracy:   1.000, loss:   0.029,   56.6s elapsed (10 steps avg.)\n",
      "Global Step 640, valid accuracy:   0.571\n",
      "INFO:tensorflow:Step 640 validation accuracy:   0.571\n",
      "Done Training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "epochs = 10\n",
    "n_steps_avg = 10\n",
    "\n",
    "FLAGS_log_dir = 'logs_chatbot/'\n",
    "\n",
    "# chat_train_set\n",
    "# chat_valid_set\n",
    "\n",
    "tune_start = timer()\n",
    "n_psets = tuner.n_sets()\n",
    "for iset, (pset, pstring) in enumerate(tuner.sets()):\n",
    "    tune_elapsed = timer() - tune_start\n",
    "    print('\\n\\nRun {}/{}, {}s elapsed'.format(iset+1, n_psets, tune_elapsed))\n",
    "    train_net(chat_train_set, chat_valid_set, epochs, pstring, **pset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 14\n"
     ]
    }
   ],
   "source": [
    "print(len(chat_train_set), len(chat_valid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "The dataset is traversed very quickly, and we trained for 10 epochs. On the training data, we can get maximum accuracy, but in validation this is only around 57% accuracy. Although this is based on only 14 validation examples, it's not so encouraging either. In any case, there is a lot of over-fitting here, even with dropout keep probability at 70%.\n",
    "\n",
    "To improve this result, the most obvious thing to do is prepare a larger dataset. With a dataset this size, you cannot expect too much predictive capability from any model. Otherwise, we also noticed that a lot of the words in the validation set had `_UNK` tokens, so either increasing the vocabulary size, or using pre-trained word embeddings may help. Another possibility is to work with a character-level model, although our expectaions/options are limited without first increasing the dataset size.\n",
    "\n",
    "nb: I also noticed a bug with the training accuracy going over 1.0. This is most likely due to one more example used in the accuracy sum than the number is normalised by. I will fix this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume Training\n",
    "\n",
    "Now re-training the same model architecture on the Resume dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Run 1/1, 0.0001233249786309898s elapsed\n",
      "\n",
      "Creating model with parameters:\n",
      "h_size          : 256\n",
      "batch_size      : 2\n",
      "rnn_layers      : 1\n",
      "dropout_keep    : 0.7\n",
      "Creating model with fresh parameters.\n",
      "\n",
      "Epoch 1\n",
      "Step     11, accuracy:   0.800, loss:   0.757,   29.9s elapsed (10 steps avg.)\n",
      "Step     21, accuracy:   0.700, loss:   0.684,   52.6s elapsed (10 steps avg.)\n",
      "Step     31, accuracy:   0.750, loss:   0.674,   77.6s elapsed (10 steps avg.)\n",
      "Step     41, accuracy:   0.700, loss:   0.670,  111.7s elapsed (10 steps avg.)\n",
      "Global Step 49, valid accuracy:   0.727\n",
      "INFO:tensorflow:Step 49 validation accuracy:   0.727\n",
      "\n",
      "Epoch 2\n",
      "Step     60, accuracy:   0.900, loss:   0.701,  190.8s elapsed (10 steps avg.)\n",
      "Step     70, accuracy:   0.600, loss:   0.694,  225.8s elapsed (10 steps avg.)\n",
      "Step     80, accuracy:   0.650, loss:   0.644,  257.6s elapsed (10 steps avg.)\n",
      "Step     90, accuracy:   0.750, loss:   0.617,  299.6s elapsed (10 steps avg.)\n",
      "Global Step 98, valid accuracy:   0.727\n",
      "INFO:tensorflow:Step 98 validation accuracy:   0.727\n",
      "\n",
      "Epoch 3\n",
      "Step    109, accuracy:   1.000, loss:   0.588,  360.9s elapsed (10 steps avg.)\n",
      "Step    119, accuracy:   0.650, loss:   0.624,  405.1s elapsed (10 steps avg.)\n",
      "Step    129, accuracy:   0.800, loss:   0.534,  438.4s elapsed (10 steps avg.)\n",
      "Step    139, accuracy:   0.600, loss:   0.628,  473.9s elapsed (10 steps avg.)\n",
      "Global Step 147, valid accuracy:   0.727\n",
      "INFO:tensorflow:Step 147 validation accuracy:   0.727\n",
      "\n",
      "Epoch 4\n",
      "Step    158, accuracy:   0.950, loss:   0.415,  561.9s elapsed (10 steps avg.)\n",
      "Step    168, accuracy:   0.750, loss:   0.482,  603.0s elapsed (10 steps avg.)\n",
      "Step    178, accuracy:   0.750, loss:   0.504,  631.7s elapsed (10 steps avg.)\n",
      "Step    188, accuracy:   0.650, loss:   0.527,  658.6s elapsed (10 steps avg.)\n",
      "Global Step 196, valid accuracy:   0.727\n",
      "INFO:tensorflow:Step 196 validation accuracy:   0.727\n",
      "\n",
      "Epoch 5\n",
      "Step    207, accuracy:   0.800, loss:   0.416,  751.0s elapsed (10 steps avg.)\n",
      "Step    217, accuracy:   0.750, loss:   0.359,  776.6s elapsed (10 steps avg.)\n",
      "Step    227, accuracy:   0.850, loss:   0.276,  810.2s elapsed (10 steps avg.)\n",
      "Step    237, accuracy:   0.850, loss:   0.230,  847.8s elapsed (10 steps avg.)\n",
      "Global Step 245, valid accuracy:   0.727\n",
      "INFO:tensorflow:Step 245 validation accuracy:   0.727\n",
      "\n",
      "Epoch 6\n",
      "Step    256, accuracy:   1.100, loss:   0.323,  946.5s elapsed (10 steps avg.)\n",
      "Step    266, accuracy:   1.000, loss:   0.408,  971.5s elapsed (10 steps avg.)\n",
      "Step    276, accuracy:   1.000, loss:   0.327, 1010.5s elapsed (10 steps avg.)\n",
      "Step    286, accuracy:   1.000, loss:   0.225, 1031.4s elapsed (10 steps avg.)\n",
      "Global Step 294, valid accuracy:   0.727\n",
      "INFO:tensorflow:Step 294 validation accuracy:   0.727\n",
      "\n",
      "Epoch 7\n",
      "Step    305, accuracy:   1.100, loss:   0.064, 1077.3s elapsed (10 steps avg.)\n",
      "Step    315, accuracy:   1.000, loss:   0.088, 1093.1s elapsed (10 steps avg.)\n",
      "Step    325, accuracy:   1.000, loss:   0.050, 1113.7s elapsed (10 steps avg.)\n",
      "Step    335, accuracy:   1.000, loss:   0.035, 1133.1s elapsed (10 steps avg.)\n",
      "Global Step 343, valid accuracy:   0.773\n",
      "INFO:tensorflow:Step 343 validation accuracy:   0.773\n",
      "\n",
      "Epoch 8\n",
      "Step    354, accuracy:   1.100, loss:   0.027, 1186.9s elapsed (10 steps avg.)\n",
      "Step    364, accuracy:   1.000, loss:   0.004, 1210.0s elapsed (10 steps avg.)\n",
      "Step    374, accuracy:   1.000, loss:   0.014, 1229.0s elapsed (10 steps avg.)\n",
      "Step    384, accuracy:   1.000, loss:   0.009, 1249.1s elapsed (10 steps avg.)\n",
      "Global Step 392, valid accuracy:   0.773\n",
      "INFO:tensorflow:Step 392 validation accuracy:   0.773\n",
      "\n",
      "Epoch 9\n",
      "Step    403, accuracy:   1.100, loss:   0.014, 1292.5s elapsed (10 steps avg.)\n",
      "Step    413, accuracy:   1.000, loss:   0.006, 1315.3s elapsed (10 steps avg.)\n",
      "Step    423, accuracy:   1.000, loss:   0.005, 1338.5s elapsed (10 steps avg.)\n",
      "Step    433, accuracy:   1.000, loss:   0.007, 1364.9s elapsed (10 steps avg.)\n",
      "Global Step 441, valid accuracy:   0.773\n",
      "INFO:tensorflow:Step 441 validation accuracy:   0.773\n",
      "\n",
      "Epoch 10\n",
      "Step    452, accuracy:   1.100, loss:   0.005, 1415.6s elapsed (10 steps avg.)\n",
      "Step    462, accuracy:   1.000, loss:   0.005, 1436.4s elapsed (10 steps avg.)\n",
      "Step    472, accuracy:   1.000, loss:   0.005, 1452.1s elapsed (10 steps avg.)\n",
      "Step    482, accuracy:   1.000, loss:   0.004, 1475.1s elapsed (10 steps avg.)\n",
      "Global Step 490, valid accuracy:   0.818\n",
      "INFO:tensorflow:Step 490 validation accuracy:   0.818\n",
      "Done Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "n_steps_avg = 10\n",
    "\n",
    "FLAGS_log_dir = 'logs_resume/'\n",
    "\n",
    "# chat_train_set\n",
    "# chat_valid_set\n",
    "\n",
    "tune_start = timer()\n",
    "n_psets = tuner.n_sets()\n",
    "for iset, (pset, pstring) in enumerate(tuner.sets()):\n",
    "    tune_elapsed = timer() - tune_start\n",
    "    print('\\n\\nRun {}/{}, {}s elapsed'.format(iset+1, n_psets, tune_elapsed))\n",
    "    train_net(resume_train_set, resume_valid_set, epochs, pstring, **pset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 23\n"
     ]
    }
   ],
   "source": [
    "print(len(resume_train_set), len(resume_valid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "For the resume dataset, we get a slightly better validation accuracy of around 82%, but again this is based on only 23 validation examples. Again we see overfitting after about 5 epochs.\n",
    "\n",
    "To improve this accuracy of this model, the same remarks for the Chatbot case apply. Very generally, the main thing that needs to be done is to increase the dataset size. If this is not an option, some features could be engineered that we know would be informative, this will help give more predictive power to the model, at the cost of making it very specific to our current case(s)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
